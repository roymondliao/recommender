{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from scipy.sparse import csr_matrix\n",
    "from tensorflow.python import debug as tfdbg\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBEL_DEVICES'] = '0, 1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path(os.environ['HOME'])\n",
    "tensorboard_path = Path(HOME) / 'tensorboard_files'\n",
    "ckpt_dir = str(HOME / 'tensorflow_ckpt' / 'CDAE')\n",
    "\n",
    "tensorboard_train_path = str(tensorboard_path / 'cdae_train')\n",
    "tensorboard_test_path = str(tensorboard_path / 'cdae_test')\n",
    "for path in [tensorboard_train_path, tensorboard_test_path, ckpt_dir]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "for path in [tensorboard_train_path, tensorboard_test_path, ckpt_dir]:\n",
    "    if len(os.listdir(path)) > 0:\n",
    "        for f in os.listdir(path):\n",
    "            if os.path.isdir(os.path.join(path, f)):\n",
    "                shutil.rmtree(os.path.join(path, f))\n",
    "            else:\n",
    "                os.remove(os.path.join(path, f))\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load video data & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 10, 0, 46, 44, 957163)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = datetime.now()\n",
    "bucket = \"<GCP bucket>\"\n",
    "bucket_prefix = '<training data path>'\n",
    "def check_date_in_storage(_date, bucket_name, bucket_prefix):\n",
    "    \n",
    "    def storage_newest_path(_bucket_name, _prefix_path):\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(_bucket_name)\n",
    "        return isinstance(bucket.get_blob(_prefix_path), blob.Blob)\n",
    "    \n",
    "    while True:\n",
    "        month = datetime.strftime(_date.date(), \"%m\")\n",
    "        day = datetime.strftime(_date.date(), \"%d\")\n",
    "        prefix_path = os.path.join(bucket_prefix,\n",
    "                                   'year={0}/month={1}/day={2}/'.format(_date.year, month, day))\n",
    "        if storage_newest_path(bucket_name, prefix_path):\n",
    "            break\n",
    "        else:\n",
    "            _date = _date - timedelta(days=1)\n",
    "    return _date\n",
    "\n",
    "get_check_date = check_date_in_storage(current_date, bucket, bucket_prefix)\n",
    "get_check_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in GCP\n",
    "training_data_perfix = os.path.join('training data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "\n",
    "validation_data_perfix = os.path.join('validation data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "testing_data_perfix = os.path.join('testing data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "click_testing_data_perfix = os.path.join('click testing data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "\n",
    "training_files = tf.gfile.ListDirectory(training_data_perfix)\n",
    "validation_files = tf.gfile.ListDirectory(validation_data_perfix)\n",
    "testing_files = tf.gfile.ListDirectory(testing_data_perfix)\n",
    "click_testing_files = tf.gfile.ListDirectory(click_testing_data_perfix)\n",
    "\n",
    "training_list = [os.path.join(training_data_perfix, f) for f in training_files if '.csv' in f]\n",
    "validation_list = [os.path.join(validation_data_perfix, f) for f in validation_files if '.csv' in f]\n",
    "testing_list = [os.path.join(testing_data_perfix, f) for f in testing_files if '.csv' in f]\n",
    "click_testing_list = [os.path.join(click_testing_data_perfix, f) for f in click_testing_files if '.csv' in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data schema\n",
    "video_data_info_perfix = os.path.join('<main path>')\n",
    "video_data_info_files = tf.gfile.ListDirectory(video_data_info_perfix)\n",
    "video_data_info_list = [os.path.join(video_data_info_perfix, f) for f in video_data_info_files if '.json' in f]\n",
    "\n",
    "with file_io.FileIO(video_data_info_list[1], mode='r') as input_f:\n",
    "    data_schema = json.load(input_f)\n",
    "\n",
    "if data_schema['CreateDate'] != get_check_date.strftime('%Y-%m-%d'):\n",
    "    sys.exit(ValueError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.DataFrame()\n",
    "for file in training_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        raw_data = pd.concat([raw_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.DataFrame()\n",
    "for file in validation_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        valid_data = pd.concat([valid_data, res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame()\n",
    "for file in testing_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        test_data = pd.concat([test_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((474632, 85), (1110843, 85))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_test_data = pd.DataFrame()\n",
    "for file in click_testing_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        click_test_data = pd.concat([click_test_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5454, 85)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load netflix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = HOME / 'ninja_project' / 'data' / 'netflix' / 'training'\n",
    "src_files = [str(training_path / f) for f in os.listdir(str(training_path)) if 'n3m' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _build_maps(src_files):\n",
    "    _user_id_map = dict()\n",
    "    _item_id_map = dict()\n",
    "\n",
    "    #src_files = [path.join(self._data_dir, f)\n",
    "    #             for f in listdir(self._data_dir)\n",
    "    #             if path.isfile(path.join(self._data_dir, f)) and f.endswith(self._extension)]\n",
    "\n",
    "    u_id = 0\n",
    "    i_id = 0\n",
    "    u_index = 0\n",
    "    i_index = 1\n",
    "    r_index = 2\n",
    "    for source_file in src_files:\n",
    "        with open(source_file, 'r') as src:\n",
    "            for line in src.readlines():\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts)<3:\n",
    "                    raise ValueError('Encountered badly formatted line in {}'.format(source_file))\n",
    "                u_id_orig = int(parts[u_index])\n",
    "                if u_id_orig not in _user_id_map:\n",
    "                    _user_id_map[u_id_orig] = u_id\n",
    "                    u_id += 1\n",
    "\n",
    "                i_id_orig = int(parts[i_index])\n",
    "                if i_id_orig not in _item_id_map:\n",
    "                    _item_id_map[i_id_orig] = i_id\n",
    "                    i_id += 1\n",
    "    return _user_id_map, _item_id_map\n",
    "\n",
    "user_id_map, item_id_map = _build_maps(src_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def _build_user_items_dict(src_file):\n",
    "    all_data = dict()\n",
    "    #for source_file in src_files:\n",
    "    with open(src_file, 'r') as f: \n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                raise ValueError('Encountered badly formatted line in {}'.format(train_sample))\n",
    "            key = user_id_map[int(parts[0])] # user_index value\n",
    "            value = item_id_map[int(parts[1])] # item_index value\n",
    "            rating = float(parts[2])\n",
    "            if key not in all_data:\n",
    "                all_data[key] = []\n",
    "            all_data[key].append((value, rating))\n",
    "    return all_data\n",
    "\n",
    "train_data = _build_user_items_dict('/home/yuyuliao/ninja_project/data/netflix/training/n3m.train.txt')\n",
    "valid_data = _build_user_items_dict('/home/yuyuliao/ninja_project/data/netflix/training/n3m.valid.txt')\n",
    "test_data = _build_user_items_dict('/home/yuyuliao/ninja_project/data/netflix/training/n3m.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_one_epoch(data, batch_size, _num_items):\n",
    "    keys = list(data.keys())\n",
    "    shuffle(keys)\n",
    "    s_ind = 0\n",
    "    e_ind = batch_size\n",
    "    mini_batch = []\n",
    "    mini_batch_index = []    \n",
    "    while e_ind < len(keys): # will have few sample droped \n",
    "        local_ind = 0 # track the local minibatch index\n",
    "        ind_users = [] # contain the indices of the minibatch\n",
    "        ind_users_remaind = []\n",
    "        ind_items = [] # contain the minor index of the input data (second column of the input data)\n",
    "        vals = [] # contain the ratings\n",
    "        indices_list = [] # create indices list of list\n",
    "\n",
    "        for ind in range(s_ind, e_ind):    \n",
    "            ind_items += [v[0] for v in data[keys[ind]]] # items index\n",
    "            ind_users += [local_ind] * len([v[0] for v in data[keys[ind]]]) # 這個人評論過 n 部電影\n",
    "            ind_users_remaind += [keys[ind]]\n",
    "            vals += [v[1] for v in data[keys[ind]]]\n",
    "            local_ind += 1\n",
    "\n",
    "        #for line in zip(ind_user, ind_items):\n",
    "        #    indices_list.append(list(line))\n",
    "\n",
    "        s_ind += batch_size\n",
    "        e_ind += batch_size\n",
    "        mini_batch = {'index': ind_users_remaind,\n",
    "                      'data': csr_matrix((vals, (ind_users, ind_items)), shape=[batch_size, _num_items]).toarray()}\n",
    "        yield mini_batch\n",
    "\n",
    "        #mini_batch = {'user_index': ind_user, 'item_index': ind_items, \n",
    "        #              'indice': indices_list, 'value': vals, 'batch_size': e_ind-s_ind}\n",
    "        #mini_batch = {'user_index': ind_user, 'item_index': ind_items,\n",
    "        #              'sparse_tensor': tf.SparseTensorValue(indices=indices_list, values=vals, dense_shape=[batch_size, _num_items])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_one_epoch_valid(data, batch_size, _num_items):\n",
    "    keys = list(data.keys())\n",
    "    weight = math.floor(len(keys) *0.50)\n",
    "    shuffle(keys[0:weight])\n",
    "    s_ind = 0\n",
    "    e_ind = batch_size\n",
    "    while e_ind < len(keys): # will have few sample droped \n",
    "        local_ind = 0 # track the local minibatch index\n",
    "        ind_user = [] # contain the indices of the minibatch\n",
    "        ind_items = [] # contain the minor index of the input data (second column of the input data)\n",
    "        vals = [] # contain the ratings\n",
    "        indices_list = [] # create indices list of list\n",
    "\n",
    "        for ind in range(s_ind, e_ind):    \n",
    "            ind_items += [v[0] for v in data[keys[ind]]] # items index\n",
    "            ind_user += [local_ind] * len([v[0] for v in data[keys[ind]]]) # 這個人評論過 n 部電影\n",
    "            vals += [v[1] for v in data[keys[ind]]]\n",
    "            local_ind += 1\n",
    "        #for line in zip(ind_user, ind_items):\n",
    "        #    indices_list.append(list(line))\n",
    "\n",
    "        s_ind += batch_size\n",
    "        e_ind += batch_size\n",
    "    \n",
    "        #mini_batch = {'user_index': ind_user, 'item_index': ind_items, \n",
    "        #              'indice': indices_list, 'value': vals, 'batch_size': e_ind-s_ind}\n",
    "        #mini_batch = {'user_index': ind_user, 'item_index': ind_items,\n",
    "        #              'sparse_tensor': tf.SparseTensorValue(indices=indices_list, values=vals, dense_shape=[batch_size, _num_items])}\n",
    "        mini_batch = csr_matrix((vals, (ind_user, ind_items)), shape=[batch_size, _num_items]).toarray()\n",
    "        yield mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sparse matrix of video id data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "def load_data_from_gcs(_raw_data_path):\n",
    "    data_prefix = os.path.join(_raw_data_path)\n",
    "    file_list = tf.gfile.ListDirectory(data_prefix)\n",
    "    raw_data = []\n",
    "    try:\n",
    "        json_files = [os.path.join(data_prefix, f) for f in file_list if '.csv' in f]\n",
    "        for file in json_files:\n",
    "            with file_io.FileIO(file, mode='rb') as input_f:\n",
    "                for line in input_f:\n",
    "                    d = json.loads(line.decode('utf-8'))\n",
    "                    raw_data.append(d)\n",
    "    except:\n",
    "        print('[ERROR] Get wrong file\\'s format, please check.')\n",
    "        exit(1)\n",
    "    return pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = os.path.join('gs://onedata-test/event/serve/adr/data_training/year=2018/month=09/day=11')\n",
    "file_list = tf.gfile.ListDirectory(data_prefix)\n",
    "csv_files = [os.path.join(data_prefix, f) for f in file_list if '.csv' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.DataFrame()\n",
    "for file in csv_files:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        raw_data = pd.concat([raw_data, res], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Use tensorflow data api - QueueRunners moudel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "data_prefix = os.path.join('<training data path>')\n",
    "file_list = tf.gfile.ListDirectory(data_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "min_after_dequeue = 100\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "tf.reset_default_graph()\n",
    "pattern = os.path.join(data_prefix, '*.csv')\n",
    "filenames = tf.train.match_filenames_once(pattern, name=None)\n",
    "filename_queue = tf.train.string_input_producer(filenames, shuffle=True, seed=202109)\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [tf.constant([], dtype=tf.string)] + [tf.constant([0], dtype=tf.int32) for _ in range(47)]\n",
    "#record_defaults = [[\"\"]] + [[0]]*47\n",
    "decode_line = tf.decode_csv(value, record_defaults=record_defaults, field_delim=',')\n",
    "uuid = tf.cast(decode_line[0], tf.string)\n",
    "reocrds = tf.reshape(decode_line[1:], shape=[-1, 47])\n",
    "\n",
    "#mb = tf.train.shuffle_batch(decode_line, batch_size=batch_size, num_threads=16, min_after_dequeue=min_after_dequeue, capacity=capacity)\n",
    "mb = tf.train.batch([uuid, reocrds],  batch_size=batch_size, num_threads=64, capacity=capacity)\n",
    "init = (tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(5):\n",
    "        #data = sess.run([decode_line, mb])\n",
    "        mini_batch = sess.run(mb)        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tensorflow data api - dataset moudel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files_list, features_name, batch_size, epochs, is_training, name):\n",
    "    def _parse_function(_line, _name):\n",
    "        num_features = len(features_name)\n",
    "        num_columns = num_features + 1\n",
    "        record_defaults = [[]] * (num_features) + [['string']]\n",
    "        decode_line = tf.decode_csv(_line, record_defaults=record_defaults, field_delim=',', name=_name)        \n",
    "        #features = dict(zip(num_features, decode_line[:num_features]))\n",
    "        index = tf.cast(decode_line[-1], tf.string)\n",
    "        features = tf.reshape(decode_line[:num_features], shape=[num_features])\n",
    "        mask = tf.cast(tf.where(tf.greater(features, 0), \n",
    "                                x=features, y=tf.zeros_like(features, dtype=tf.float32)), dtype=tf.bool)        \n",
    "        return index, features, mask\n",
    "\n",
    "    def _input_fn():\n",
    "        #match_pattern = tf.placeholder(dtype=tf.string)\n",
    "        #filenames = tf.train.match_filenames_once(pattern=match_pattern, name=None)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(files_list)\n",
    "        dataset = dataset.flat_map(lambda filename: (tf.data.TextLineDataset(filename).\\\n",
    "                                                     skip(1).\\\n",
    "                                                     map(lambda line: _parse_function(line, name), \n",
    "                                                         num_parallel_calls=multiprocessing.cpu_count())))\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(buffer_size=256)\n",
    "            #dataset = dataset.repeat(epochs)    \n",
    "        dataset = dataset.prefetch(buffer_size=batch_size * 100) # n = 元素個數 / Batch size \n",
    "        dataset = dataset.batch(batch_size)                                            \n",
    "        return dataset\n",
    "    return _input_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Graph().as_default() as g:    \n",
    "    with g.device('/cpu:0'):\n",
    "        with tf.name_scope('DataPipelines'):\n",
    "            train_dataset = load_data(files_list=training_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=1, name='train')    \n",
    "            iterator = tf.data.Iterator.from_structure(train_dataset().output_types, \n",
    "                                                       train_dataset().output_shapes)    \n",
    "            batch_index, batch_features, batch_mask = iterator.get_next()\n",
    "            train_init_op = iterator.make_initializer(train_dataset())            \n",
    "            \n",
    "with tf.Session(graph=g, config=config) as sess:\n",
    "    total_train_steps = 99999\n",
    "    for epoch in range(1):        \n",
    "        user_v_index_dict = {}\n",
    "        tic = time.time()        \n",
    "        sess.run(train_init_op)\n",
    "        for i in range(total_train_steps):                \n",
    "            try:\n",
    "                train_labels, train_features, train_mask = sess.run([batch_index, batch_features, batch_mask])  \n",
    "                for k, v in enumerate(train_labels):\n",
    "                    user_v_index_dict[v] = (NUM_BATCH_SIZE * i) + k\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Finished, time:{}'.format(time.time() - tic))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraments setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITEMS = len(data_schema['DataSchema'][:-1])\n",
    "\n",
    "# raw_data.shape[0], 不使用 user 的總數是因為這樣還需要預載 user data，用 user 是否看過該影片來當作 user information\n",
    "N_USERS = len(data_schema['DataSchema'][:-1]) \n",
    "\n",
    "# Parameters setting\n",
    "NUM_EPOCHS = 1\n",
    "NUM_BATCH_SIZE = 128\n",
    "CORRUPTION_LEVEL = 0.333\n",
    "DROPOUT_PROB = 0.5\n",
    "N_HIDDEN = 50\n",
    "LAYERS = [N_ITEMS, N_HIDDEN, N_ITEMS]\n",
    "        \n",
    "# AdaGrad Parameters\n",
    "ADA_BETA = 1.0\n",
    "START_LEARNING_RATE = 0.00001\n",
    "\n",
    "# Regularization Parameter\n",
    "REG_LAMBDA = 0.001\n",
    "\n",
    "gradient_clip = 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard representations of weight version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(node_in, node_out, constant=1):\n",
    "    low = -constant * np.sqrt( 6 / (node_in + node_out))\n",
    "    high = constant * np.sqrt( 6 / (node_in + node_out))\n",
    "    return tf.random_uniform((node_in, node_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "def activation(x, fn, name=None):\n",
    "    if fn == 'selu':\n",
    "        return tf.nn.selu(x, name)\n",
    "    elif fn == 'relu':\n",
    "        return tf.nn.relu(x, name)\n",
    "    elif fn == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x, name)\n",
    "    elif fn == 'relu6':\n",
    "        return tf.nn.relu6(x, name)\n",
    "    elif fn == 'elu':\n",
    "        return tf.nn.elu(x, name)\n",
    "    elif fn == 'lrelu':\n",
    "        return tf.nn.leaky_relu(x, 0.2, name)\n",
    "    elif fn == None:\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linearity type')\n",
    "        \n",
    "def initialize_weights_v1(n_users, n_items, n_hidden):\n",
    "    start_time = time.time()\n",
    "    print('Initial weights...')\n",
    "    all_weights = dict()\n",
    "    W = xavier_init(n_items, n_hidden)\n",
    "    W_prime = xavier_init(n_items, n_hidden)\n",
    "    with tf.name_scope('Weights'):\n",
    "        all_weights['W'] = [tf.Variable(W[i]) for i in range(n_items)]\n",
    "        all_weights['W_prime'] = [tf.Variable(W_prime[i]) for i in range(n_items)]\n",
    "        tf.summary.histogram(name='W', values=all_weights['W'])\n",
    "        tf.summary.histogram(name='W_prime', values=all_weights['W_prime'])\n",
    "        \n",
    "    with tf.name_scope('Biases'):\n",
    "        all_weights['b'] = tf.Variable(tf.zeros([1, n_hidden], dtype=tf.float32))\n",
    "        all_weights['b_prime'] = [tf.Variable(tf.constant(0, dtype=tf.float32), dtype=tf.float32) for _ in range(n_items)]\n",
    "        tf.summary.histogram(name='b', values=all_weights['b'])\n",
    "        tf.summary.histogram(name='b_prime', values=all_weights['b_prime'])\n",
    "            \n",
    "    with tf.name_scope('UserSpecific'):  \n",
    "        all_weights['V'] = [tf.Variable(tf.zeros([n_hidden])) for _ in range(n_users)]\n",
    "        tf.summary.histogram(name='V', values=all_weights['V'])\n",
    "        #all_weights['V'] = tf.get_variable(name='V', \n",
    "        #                                   initializer=tf.truncated_normal(shape=[n_users, n_hidden], \n",
    "        #                                                                   mean=0, stddev=0.01),\n",
    "        #                                   dtype=tf.float32)        \n",
    "        #all_weights['V'] = tf.Variable(name='V', initial_value=W, expected_shape=(N_USERS, N_HIDDEN))\n",
    "        \n",
    "    print('Initial weights done! Processing time:{}'.format(time.time() - start_time))\n",
    "    return all_weights        \n",
    "    \n",
    "def initialize_weights_v2(layers, n_users, n_hidden):\n",
    "    weights = {}\n",
    "    with tf.variable_scope('hyperparameters', reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope('Weights'):\n",
    "            xavier_init = tf.contrib.layers.xavier_initializer(seed=202109)\n",
    "            for i in range(len(layers) - 1):\n",
    "                layer_name = 'layer%s' % i\n",
    "                weights['W' + str(i + 1)] = tf.get_variable(name=\"W\" + str(i + 1), \n",
    "                                                            shape=(layers[i], layers[i + 1]), \n",
    "                                                            initializer=xavier_init)                \n",
    "        with tf.name_scope('Biases'):        \n",
    "            for i in range(len(layers) - 1):\n",
    "                layer_name = 'layer%s' % i    \n",
    "                weights['b'+ str(i + 1)] = tf.get_variable(name=\"b\" + str(i + 1), \n",
    "                                                           shape=(layers[i + 1], ), \n",
    "                                                           initializer=tf.zeros_initializer())\n",
    "        # User Specific 用 embedding lookup 的方式來取代\n",
    "        with tf.name_scope('UserSpecific'):\n",
    "            #weights['V'] = tf.get_variable(name='V', shape=(n_users, n_hidden), initializer=xavier_init)\n",
    "            weights['V'] = tf.get_variable(name=\"V\", initializer=tf.truncated_normal(shape=[n_users, n_hidden], \n",
    "                                                                                     mean=0, stddev=0.03), \n",
    "                                           dtype=tf.float32)\n",
    "            #weights['V'] = [tf.Variable(tf.zeros([n_hidden])) for _ in range(n_users)]\n",
    "    return weights\n",
    "\n",
    "def initialize_weights_v3(layers_structure, n_users, n_hidden):\n",
    "    weights = dict()\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer(seed=202109)\n",
    "    with tf.variable_scope('hyperparameters', reuse=tf.AUTO_REUSE):\n",
    "        for i in range(len(layers_structure) - 1):\n",
    "            layer_name = 'layer%s' % i\n",
    "            with tf.name_scope('Weights'):        \n",
    "                weights['W' + str(i + 1)] = tf.get_variable(name=(\"W\" + str(i)), \n",
    "                                                            #shape=(layers_structure[i], layers_structure[i + 1]),\n",
    "                                                            #initializer=xavier_init, \n",
    "                                                            initializer=tf.truncated_normal(mean=0, stddev=0.01,\n",
    "                                                                                           shape=(layers_structure[i], layers_structure[i + 1])),\n",
    "                                                            dtype=tf.float32)\n",
    "                tf.summary.histogram(name=layer_name + '/Weights', values=weights['W' + str(i + 1)])\n",
    "                \n",
    "            with tf.name_scope('Biases'):        \n",
    "                weights['b' + str(i + 1)] = tf.get_variable(name=(\"b\" + str(i)), \n",
    "                                                            initializer=tf.zeros(shape=layers_structure[i + 1]), \n",
    "                                                            dtype=tf.float32)\n",
    "                tf.summary.histogram(name=layer_name + '/Biases', values=weights['b' + str(i + 1)])\n",
    "                \n",
    "        with tf.name_scope('UserSpecific'):\n",
    "            weights['V'] = tf.get_variable(name=\"V\", \n",
    "                                           initializer=tf.truncated_normal(shape=[n_users, n_hidden], mean=0, stddev=0.03),\n",
    "                                           dtype=tf.float32)            \n",
    "            tf.summary.histogram(name=layer_name + '/UserSp', values=weights['V'])\n",
    "            \n",
    "    return weights\n",
    "\n",
    "def DAE(y, v, weights, n_items, n_users, n_hidden, dropout_prob, transfer_fn, output_fn):\n",
    "    with tf.name_scope('DAE'):\n",
    "        y_tilde = tf.nn.dropout(y, dropout_prob) \n",
    "        z = activation(tf.add_n([tf.matmul(y, weights['W']),\n",
    "                                 tf.reshape(v, [1, n_hidden]), \n",
    "                                 tf.reshape(weights['b'], [1, n_hidden])]), transfer_fn) \n",
    "        y_hat = activation(tf.add(tf.matmul(z, tf.transpose(weights['W_prime'])), \n",
    "                                  tf.reshape(weights['b_prime'], [1, n_items])), output_fn)                    \n",
    "        # tf.constant(y, dtype=tf.float32, shape=[1, n_items])\n",
    "        # z = activation(tf.add_n([tf.matmul(y, weights['W1']),\n",
    "        #                          tf.reshape(u, [1, n_hidden]), \n",
    "        #                          tf.reshape(weights['b1'], [1, n_hidden])]), transfer_fn) \n",
    "        # y_hat = activation(tf.add(tf.matmul(z, weights['W2']), \n",
    "        #                           tf.reshape(weights['b2'], [1, n_items])), output_fn)        \n",
    "    return y_tilde, y_hat\n",
    "        \n",
    "def DAE_V2(corrupted_y, v, layers_structure, weights, n_items, n_users, n_hidden, dropout_prob, \n",
    "           transfer_fn, output_fn):            \n",
    "    layers = int(len(layers_structure) / 2)\n",
    "    y_tilde = tf.nn.dropout(corrupted_y, dropout_prob)\n",
    "    with tf.name_scope('Encoder'):\n",
    "        for layer in range(1, layers + 1):\n",
    "            z = activation(tf.add(tf.add(tf.matmul(y_tilde, weights['W' + str(layer)]), v), \n",
    "                                  tf.reshape(weights['b' + str(layer)], [1, n_hidden])), transfer_fn)\n",
    "            #z = activation(tf.add_n([tf.matmul(y_tilde, weights['W' + str(layer)]), \n",
    "            #                         v, tf.reshape(weights['b' + str(layer)], [1, n_hidden])]), \n",
    "            #               transfer_fn)   \n",
    "    with tf.name_scope('Decoder'):\n",
    "        for layer in range(layers + 1, 2 * layers + 1):        \n",
    "            if layer == 2 * layers:\n",
    "                y_hat = activation(tf.add(tf.matmul(z, weights['W' + str(layer)]), \n",
    "                                      tf.reshape(weights['b' + str(layer)], [n_items])), output_fn, name='Prediction') \n",
    "            else:\n",
    "                y_hat = activation(tf.add(tf.matmul(z, weights['W' + str(layer)]), \n",
    "                                      tf.reshape(weights['b' + str(layer)], [n_items])), output_fn)\n",
    "    return y_tilde, y_hat\n",
    "\n",
    "def get_loss(corrupted_y, y_hat, weights, layers_structure, v_user, reg_lambda, _augmented_o_mask, loss_fn):\n",
    "    real_batch_size = tf.cast(tf.shape(corrupted_y)[0], tf.int32)\n",
    "    U = tf.cast(real_batch_size, tf.float32)\n",
    "    reg_loss = tf.constant(0, dtype=tf.float32)\n",
    "    \n",
    "    if loss_fn == 'cross_entropy_loss':\n",
    "        p = tf.nn.sigmoid(y_hat)\n",
    "        #temp_p = tf.clip_by_value(y_hat, 1e-8, tf.reduce_max(y_hat))\n",
    "        first_loss = -1 * tf.multiply(corrupted_y, tf.log(p)) - tf.multiply((1 - corrupted_y) , tf.log(1 - p))   \n",
    "        first_loss = np.multiply(first_loss, _augmented_o_mask)\n",
    "        tmpfirst_loss = tf.reduce_sum(first_loss) / U\n",
    "        for itr in range(1, len(layers_structure)):        \n",
    "            reg_loss = tf.add(reg_loss, tf.add(tf.nn.l2_loss(weights['W' + str(itr)]), \n",
    "                                               tf.nn.l2_loss(weights['b' + str(itr)])))            \n",
    "        reg_loss = reg_lambda * 0.5 * (reg_loss + tf.nn.l2_loss(batch_V))\n",
    "        loss = tmpfirst_loss + reg_loss \n",
    "        \n",
    "    elif loss_fn == 'square_loss':              \n",
    "        square_loss = tf.nn.l2_loss(tf.multiply(tf.subtract(corrupted_y, y_hat), _augmented_o_mask))\n",
    "        loss = square_loss / U\n",
    "        \n",
    "    elif loss_fn == 'log_loss':    \n",
    "        logloss = tf.multiply(tf.log(1 + tf.exp(tf.multiply(-corrupted_y, y_hat))), _augmented_o_mask)\n",
    "        loss = tf.reduce_sum(logloss) / U\n",
    "        \n",
    "    elif loss_fn == 'hinge_loss':\n",
    "        hingeloss = tf.multiply(1 - tf.multiply(corrupted_y, y_hat), _augmented_o_mask)\n",
    "        loss = tf.maximum(0.0, tf.reduce_sum(hingeloss)) / U\n",
    "        \n",
    "    else: \n",
    "        raise ValueError('Unknown non-linearity type')        \n",
    "    return loss\n",
    "\n",
    "def augmented_0_set(ith_data, neg_sample_ratio=5):\n",
    "    pos_examples_indices = np.where(ith_data > 0)[0]\n",
    "    neg_examples_indices = np.where(ith_data == 0)[0]\n",
    "    aug_set = {index for index in pos_examples_indices} \n",
    "    if len(neg_examples_indices) <= neg_sample_ratio * len(pos_examples_indices):\n",
    "        for index in neg_examples_indices:\n",
    "            aug_set.add(index)\n",
    "    else:\n",
    "        for index in np.random.permutation(neg_examples_indices)[0:neg_sample_ratio * len(pos_examples_indices)]:\n",
    "            aug_set.add(index)\n",
    "    return aug_set\n",
    "\n",
    "def batch_augmented_O_set(ith_data, neg_sample_ratio=5):\n",
    "    batch_augmented_O_mask = []\n",
    "    for i in range(ith_data.shape[0]):\n",
    "        pos_examples_indices = np.where(ith_data[i] > 0)[0]\n",
    "        neg_examples_indices = np.where(ith_data[i] == 0)[0]\n",
    "        aug_set = {index for index in pos_examples_indices} \n",
    "        if len(neg_examples_indices) <= neg_sample_ratio * len(pos_examples_indices):\n",
    "            for index in neg_examples_indices:\n",
    "                aug_set.add(index)\n",
    "        else:\n",
    "            for index in np.random.permutation(neg_examples_indices)[0:neg_sample_ratio * len(pos_examples_indices)]:\n",
    "                aug_set.add(index)\n",
    "        augmented_O_mask = np.zeros(ith_data.shape[1])\n",
    "        augmented_O_mask[list(aug_set)] = 1         \n",
    "        batch_augmented_O_mask.append(augmented_O_mask)    \n",
    "    return np.array(batch_augmented_O_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(loss):  \n",
    "    optimizer = tf.train.AdagradOptimizer(ADA_LEARNING_RATE, ADA_BETA)\n",
    "    grad_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in grad_and_vars]        \n",
    "    cost_gradient_apply = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default() as g:\n",
    "    tf.set_random_seed(202109)\n",
    "    with g.device('/cpu:0'):\n",
    "            train_dataset = load_data(files_list=training_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=1, name='train')    \n",
    "            valid_dataset = load_data(files_list=validation_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=0, name='valid') \n",
    "            iterator = tf.data.Iterator.from_structure(train_dataset().output_types, \n",
    "                                                   train_dataset().output_shapes)    \n",
    "            batch_index, batch_features, batch_mask = iterator.get_next()\n",
    "            train_init_op = iterator.make_initializer(train_dataset())        \n",
    "            valid_init_op = iterator.make_initializer(valid_dataset())\n",
    "            \n",
    "    with g.device('/gpu:0'):\n",
    "        with tf.name_scope('Placeholder'):\n",
    "            Y = tf.placeholder(tf.float32, [None, N_ITEMS], name='Inputs')\n",
    "            v_node = tf.placeholder(tf.int32, [None, N_ITEMS], name='UserNode')\n",
    "            corrupted_mask = tf.placeholder(tf.int32, [None, N_ITEMS], name='Mask')\n",
    "            #input_ids = tf.placeholder(tf.int32, [None], name='UserId')\n",
    "            #batch_size = tf.cast(tf.shape(Y)[0], tf.int32)\n",
    "            augmented_O_mask = tf.placeholder(tf.float32, [None, N_ITEMS], name='AugmentedMask')\n",
    "            global_step = tf.Variable(tf.constant(0), trainable=False, name='GlobalStepCounter') \n",
    "        \n",
    "        # Compute graph\n",
    "        corrupted_Y = tf.multiply(Y, tf.cast(corrupted_mask, tf.float32))            \n",
    "        with tf.variable_scope('Hyperparameters', reuse=tf.AUTO_REUSE):\n",
    "            all_weights = initialize_weights_v3(layers_structure=LAYERS, \n",
    "                                                n_users=N_USERS, \n",
    "                                                n_hidden=N_HIDDEN)                            \n",
    "        \n",
    "        with tf.name_scope('DAE'):\n",
    "            #batch_V = tf.reshape(tf.gather(all_weights['V'], input_ids, name='get_V'), [batch_size, N_HIDDEN])\n",
    "            batch_V = tf.matmul(tf.cast(v_node, tf.float32), all_weights['V'])\n",
    "            Y_tilde, Y_hat = DAE_V2(corrupted_y=corrupted_Y, v=batch_V, layers_structure=LAYERS, \n",
    "                                    weights=all_weights, n_items=N_ITEMS, n_users=N_USERS, n_hidden=N_HIDDEN, \n",
    "                                    dropout_prob=DROPOUT_PROB, transfer_fn='sigmoid', output_fn='sigmoid')\n",
    "    \n",
    "        with tf.name_scope('Loss'):\n",
    "            final_loss = get_loss(corrupted_y=corrupted_Y, y_hat=Y_hat, weights=all_weights, \n",
    "                                  layers_structure=LAYERS, v_user=batch_V, reg_lambda=REG_LAMBDA,\n",
    "                                  _augmented_o_mask=augmented_O_mask, loss_fn='square_loss')\n",
    "            tf.summary.scalar(name='Loss', tensor=final_loss)\n",
    "            \n",
    "    #with g.device('/gpu:0'):        \n",
    "        with tf.name_scope('Optimizer'):       \n",
    "            learning_rate = tf.train.exponential_decay(START_LEARNING_RATE, global_step, 1000, 1.96, staircase=False)\n",
    "            tf.summary.scalar(name='LR', tensor=learning_rate)\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate, ADA_BETA)\n",
    "            \n",
    "            grad_and_vars = optimizer.compute_gradients(final_loss)\n",
    "            #gradients, variables = zip(*optimizer.compute_gradients(final_loss))\n",
    "            \n",
    "            gradients = [x[0] for x in grad_and_vars]\n",
    "            variables = [x[1] for x in grad_and_vars]\n",
    "            if gradient_clip > 0:\n",
    "                clipped, _ = tf.clip_by_global_norm(gradients, gradient_clip)                \n",
    "            else:\n",
    "                clipped = [tf.clip_by_value(grad, -1., 1.) for grad in gradients]\n",
    "                #clipped = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in grad_and_vars]\n",
    "            \n",
    "            cost_gradient_apply = optimizer.apply_gradients(zip(clipped, variables), global_step=global_step)\n",
    "            #cost_gradient_apply = optimizer.apply_gradients(grads_and_vars=capped_gvs, global_step=global_step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialing all Variables...\n",
      "Initialing all variables done! Process time:0.06098985672s\n",
      "Start Training...\n",
      "=== Epoch:  0 ===\n",
      "Training-Iteration: 1000, loss: 11.09694900\n",
      "Training-Iteration: 2000, loss: 11.09650890\n",
      "Training-Iteration: 3000, loss: 11.12478720\n",
      "Training-Iteration: 4000, loss: 11.11947479\n",
      "Training-Iteration: 5000, loss: 11.12444935\n",
      "Training-Iteration: 6000, loss: 11.11838916\n",
      "Training-Iteration: 7000, loss: 11.09638240\n",
      "Training-Iteration: 8000, loss: 11.06739746\n",
      "Iter: 8862 is end of line.\n",
      "Testing-Iteration: 1000, loss: 9.71549552\n",
      "Testing-Iteration: 2000, loss: 9.57642125\n",
      "Testing-Iteration: 3000, loss: 9.47565736\n",
      "Iter: 3709 is end of line.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'/home/yuyuliao/tensorflow_ckpt/CDAE/export/saved_model.pb'\n",
      "#### Epoch:0, Train Cost:11.02598670, Valid Cost:9.41682449, Processing time:240.62308s\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=g, config=config) as sess:\n",
    "    export_dir = os.path.join(ckpt_dir, 'export')\n",
    "    \n",
    "    random_user_set = np.random.permutation(N_USERS)\n",
    "    old_avg_cost = None\n",
    "    model_improved = True\n",
    "    epoch = 0\n",
    "    total_train_steps = 999999 #math.floor(N_USERS / NUM_BATCH_SIZE)\n",
    "    total_valid_steps = 999999\n",
    "    \n",
    "    # Best validation loss seen so far.\n",
    "    best_valid_loss = 99999\n",
    "    # Iteration-number for last improvement to validation loss.\n",
    "    last_improvement = 0\n",
    "    # Stop optimization if no improvement found in this many iterations.\n",
    "    require_improvement = 3\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_train_path, graph=sess.graph,\n",
    "                                         filename_suffix='CDAE')\n",
    "    test_writer = tf.summary.FileWriter(tensorboard_test_path, graph=sess.graph,\n",
    "                                        filename_suffix='CDAE')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    start_time = time.time()\n",
    "    print('Initialing all Variables...')\n",
    "    sess.run(init)\n",
    "    print('Initialing all variables done! Process time:{:.10}s'.format(time.time() - start_time))\n",
    "    print('Start Training...')           \n",
    "    \n",
    "    while epoch < NUM_EPOCHS:\n",
    "        print('=== Epoch: {:2d} ==='.format(epoch))\n",
    "        train_sum_cost = 0\n",
    "        valid_sum_cost = 0\n",
    "        #user_v_index_dict= dict()\n",
    "\n",
    "        start_time = time.time()\n",
    "        sess.run(train_init_op)\n",
    "        for i in range(total_train_steps): \n",
    "            try:\n",
    "                train_labels, train_features, train_user_node = sess.run([batch_index, batch_features, batch_mask])\n",
    "                #v_index = list(range((train_features.shape[0] * i), (train_features.shape[0] * (i + 1))))\n",
    "                #for k, v in enumerate(train_labels):\n",
    "                #    user_v_index_dict[v] = (NUM_BATCH_SIZE * i) + k\n",
    "                mask_corruption = np.random.binomial(1, 1 - CORRUPTION_LEVEL, (train_features.shape[0], N_ITEMS))\n",
    "                mask_corruption_np = np.array(mask_corruption, dtype=np.float32) \n",
    "                maskt_augmented_O = batch_augmented_O_set(train_features)\n",
    "            \n",
    "                train_loss, _ = sess.run([final_loss, cost_gradient_apply], \n",
    "                                         feed_dict={Y: train_features, \n",
    "                                                    corrupted_mask: mask_corruption_np,\n",
    "                                                    v_node: train_user_node,\n",
    "                                                    #input_ids: v_index,\n",
    "                                                    augmented_O_mask: maskt_augmented_O})\n",
    "                train_sum_cost += train_loss\n",
    "                if i % 1000 == 0 and i > 0:\n",
    "                    summary = sess.run(merged_summary, feed_dict={Y: train_features, \n",
    "                                                                  corrupted_mask: mask_corruption_np,\n",
    "                                                                  v_node: train_user_node,\n",
    "                                                                  augmented_O_mask: maskt_augmented_O})    \n",
    "                    train_writer.add_summary(summary, epoch * total_train_steps + i)\n",
    "                    print('Training-Iteration: {:4d}, loss: {:.8f}'.format(i, train_sum_cost / i))\n",
    "                    if np.isnan(train_loss):\n",
    "                        break                    \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Iter: {:4d} is end of line.'.format(i))\n",
    "                break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        train_avg_cost = train_sum_cost / i\n",
    "        #summary = sess.run(merged_summary, feed_dict={Y: train_features, \n",
    "        #                                              corrupted_mask: mask_corruption_np,\n",
    "        #                                              v_node: train_user_node,\n",
    "        #                                              #input_ids: v_index,\n",
    "        #                                              augmented_O_mask: maskt_augmented_O})    \n",
    "        #train_writer.add_summary(summary, epoch) # epoch * total_train_steps + i\n",
    "        \n",
    "        # Validation\n",
    "        sess.run(valid_init_op)\n",
    "        for j in range(total_valid_steps):         \n",
    "            try: \n",
    "                valid_labels, valid_features, valid_user_node = sess.run([batch_index, batch_features, batch_mask])\n",
    "                #v_index = list(range((valid_features.shape[0] * j), (valid_features.shape[0] * (j + 1))))\n",
    "                mask_corruption = np.random.binomial(1, 1 - CORRUPTION_LEVEL, (valid_features.shape[0], N_ITEMS))\n",
    "                mask_corruption_np = np.array(mask_corruption, dtype=np.float32) \n",
    "                maskt_augmented_O = batch_augmented_O_set(valid_features)\n",
    "            \n",
    "                valid_loss, _ = sess.run([final_loss, cost_gradient_apply], \n",
    "                                         feed_dict={Y: valid_features, \n",
    "                                                    corrupted_mask: mask_corruption_np,\n",
    "                                                    v_node: valid_user_node,\n",
    "                                                    #input_ids: v_index,\n",
    "                                                    augmented_O_mask: maskt_augmented_O})\n",
    "                valid_sum_cost += valid_loss\n",
    "                if j % 1000 == 0 and j > 0:\n",
    "                    print('Testing-Iteration: {:4d}, loss: {:.8f}'.format(j, valid_sum_cost / j))\n",
    "                    valid_summary = sess.run(merged_summary, feed_dict={Y: valid_features, \n",
    "                                                                        corrupted_mask: mask_corruption_np,\n",
    "                                                                        v_node: valid_user_node,\n",
    "                                                                        augmented_O_mask: maskt_augmented_O})    \n",
    "                    test_writer.add_summary(valid_summary, epoch * total_train_steps + i)\n",
    "                    if np.isnan(valid_loss):\n",
    "                        break  \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Iter: {:4d} is end of line.'.format(j))\n",
    "                break\n",
    "                \n",
    "        end_time = time.time()\n",
    "        valid_avg_cost = valid_sum_cost / j\n",
    "        #valid_summary = sess.run(merged_summary, feed_dict={Y: valid_features, \n",
    "        #                                                    corrupted_mask: mask_corruption_np,\n",
    "        #                                                    v_node: valid_user_node,\n",
    "        #                                                    #input_ids: v_index,\n",
    "        #                                                    augmented_O_mask: maskt_augmented_O})    \n",
    "        #test_writer.add_summary(valid_summary, epoch) # epoch * total_train_steps + i\n",
    "        \n",
    "        # If validation accuracy is an improvement over best-known.\n",
    "        if valid_avg_cost < best_valid_loss:\n",
    "            best_valid_loss = valid_avg_cost                            \n",
    "            last_improvement = epoch\n",
    "            #saver.save(sess, os.path.join(ckpt_dir, 'DeepRecommender.ckpt'), global_step=epoch * total_train_steps + i)\n",
    "            inputs = g.get_tensor_by_name('Placeholder/Inputs:0')\n",
    "            inputs_user_node = g.get_tensor_by_name('Placeholder/UserNode:0')\n",
    "            inputs_mask = g.get_tensor_by_name('Placeholder/Mask:0')\n",
    "            inputs_augmented_mask = g.get_tensor_by_name('Placeholder/AugmentedMask:0')\n",
    "            prediction = g.get_tensor_by_name('DAE/Decoder/Prediction:0')            \n",
    "            \n",
    "            model_input = {\n",
    "                'inputs': tf.saved_model.utils.build_tensor_info(inputs),\n",
    "                'v_node': tf.saved_model.utils.build_tensor_info(inputs_user_node),\n",
    "                'corrupted_mask': tf.saved_model.utils.build_tensor_info(inputs_mask),\n",
    "                'augmented_O_mask': tf.saved_model.utils.build_tensor_info(inputs_augmented_mask)\n",
    "            }\n",
    "            \n",
    "            model_output = {\n",
    "                'outputs': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "            }\n",
    "            # Build signature definition\n",
    "            signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                inputs=model_input,\n",
    "                outputs=model_output,\n",
    "                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "            )\n",
    "            \n",
    "            if os.path.exists(export_dir):\n",
    "                shutil.rmtree(export_dir, ignore_errors=True)\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "            builder.add_meta_graph_and_variables(\n",
    "                sess, [tf.saved_model.tag_constants.SERVING], \n",
    "                signature_def_map={\n",
    "                    tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature\n",
    "                }, strip_default_attrs=True)\n",
    "            builder.save()\n",
    "            improved_str = '*'\n",
    "\n",
    "        if epoch - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization.\")\n",
    "            break\n",
    "            \n",
    "        #model_improved = True if old_avg_cost == None or old_avg_cost > avg_cost else False       \n",
    "        print('#### Epoch:{}, Train Cost:{:.8f}, Valid Cost:{:.8f}, Processing time:{:.5f}s'.format(epoch, \n",
    "                                                                                                     train_avg_cost,\n",
    "                                                                                                     valid_avg_cost,\n",
    "                                                                                                     end_time - start_time))\n",
    "        epoch += 1                 \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SavedModel module to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path(os.environ['HOME'])\n",
    "tensorboard_path = Path(HOME) / 'tensorboard_files'\n",
    "ckpt_dir = str(HOME / 'tensorflow_ckpt' / 'CDAE')\n",
    "export_dir = os.path.join(ckpt_dir, 'export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'/home/yuyuliao/tensorflow_ckpt/CDAE/export/variables/variables'\n",
      "Num of iteration:3709, total predicted count: 474752\n",
      "CPU times: user 7min 22s, sys: 9.27 s, total: 7min 32s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "pred_config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "with tf.Graph().as_default() as pred_g: \n",
    "    test_dataset = load_data(files_list=validation_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                              batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=0, name='test') \n",
    "    test_iterator = tf.data.Iterator.from_structure(test_dataset().output_types, \n",
    "                                               test_dataset().output_shapes)    \n",
    "    batch_index, batch_features, batch_mask = test_iterator.get_next()\n",
    "    test_init_op = test_iterator.make_initializer(test_dataset())    \n",
    "\n",
    "    \n",
    "with tf.Session(config=pred_config, graph=pred_g) as pred_sess:\n",
    "    total_test_steps = 99999\n",
    "    meta_graph_def = tf.saved_model.loader.load(pred_sess, [tf.saved_model.tag_constants.SERVING], \n",
    "                                                export_dir)\n",
    "    signature = meta_graph_def.signature_def\n",
    "    signature_dict = signature[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "    x_tensor_name = signature_dict.inputs['inputs'].name\n",
    "    v_node_tensor_name = signature_dict.inputs['v_node'].name\n",
    "    corrupted_mask_tensor_name = signature_dict.inputs['corrupted_mask'].name\n",
    "    augmented_O_mask_tensor_name = signature_dict.inputs['augmented_O_mask'].name\n",
    "    y_tensor_name = signature_dict.outputs['outputs'].name\n",
    "    pred_sess.run(test_init_op)\n",
    "    #graph = tf.get_default_graph()\n",
    "    #graph_nodes_name = [n.name for n in graph.as_graph_def().node]\n",
    "    prediction_recoder = []\n",
    "    X = pred_sess.graph.get_tensor_by_name(x_tensor_name)\n",
    "    v_node = pred_sess.graph.get_tensor_by_name(v_node_tensor_name)\n",
    "    corrupted_mask = pred_sess.graph.get_tensor_by_name(corrupted_mask_tensor_name)\n",
    "    augmented_O_mask = pred_sess.graph.get_tensor_by_name(augmented_O_mask_tensor_name)    \n",
    "    Y = pred_sess.graph.get_tensor_by_name(y_tensor_name)\n",
    "    for _ in range(total_test_steps):\n",
    "        try:\n",
    "            test_labels, test_features, test_user_node = pred_sess.run([batch_index, batch_features, batch_mask])\n",
    "            mask_corruption = np.random.binomial(1, 1 - CORRUPTION_LEVEL, (test_features.shape[0], N_ITEMS))\n",
    "            mask_corruption_np = np.array(mask_corruption, dtype=np.float32) \n",
    "            maskt_augmented_O = batch_augmented_O_set(test_features)\n",
    "            \n",
    "            decode_test_labels = [l.decode('utf-8') for l in test_labels]            \n",
    "            predicted_result = pred_sess.run(Y, feed_dict={\n",
    "                X: test_features, \n",
    "                v_node: test_user_node, \n",
    "                corrupted_mask: mask_corruption_np, \n",
    "                augmented_O_mask: maskt_augmented_O\n",
    "            })  \n",
    "            prediction_recoder.append(list(zip(decode_test_labels, np.round(predicted_result, 6))))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "print('Num of iteration:{}, total predicted count: {}'.format(len(prediction_recoder), \n",
    "                                                              len(prediction_recoder) * NUM_BATCH_SIZE))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed representations of weight version\n",
    "From another point of view, W i and V u can be seen as the distributed representations of item i and \n",
    "user u respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(node_in, node_out, constant=1):\n",
    "    low = -constant * np.sqrt( 6 / (node_in + node_out))\n",
    "    high = constant * np.sqrt( 6 / (node_in + node_out))\n",
    "    return tf.random_uniform((node_in, node_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "class CDAE:\n",
    "    def __init__(self, input_class, sparse=True):\n",
    "        #self.input_data = input_class(sparse=sparse)\n",
    "        self.input_data = input_class\n",
    "        self.sparse = sparse\n",
    "        self.n_items  = self.input_data.get_n_items()\n",
    "        self.n_users  = self.input_data.get_n_users()\n",
    "\n",
    "    def _init_variables(self, n_hidden=50):\n",
    "        start_time = time.time()\n",
    "        print('Initial weights...')\n",
    "        W = xavier_init(self.n_items, n_hidden)\n",
    "        W_prime = xavier_init(self.n_items, n_hidden)\n",
    "        all_weights = dict()\n",
    "        all_weights['W'] = [tf.Variable(W[i]) for i in range(self.n_items)]\n",
    "        all_weights['W_prime'] = [tf.Variable(W_prime[i]) for i in range(self.n_items)]\n",
    "        all_weights['b'] = tf.Variable(tf.zeros([1, n_hidden], dtype=tf.float32))\n",
    "        all_weights['b_prime'] = [tf.Variable(tf.constant(0, dtype=tf.float32), dtype=tf.float32) for _ in range(self.n_items)]\n",
    "        all_weights['V'] = [tf.Variable(tf.zeros([n_hidden])) for _ in range(self.n_users)]\n",
    "        print('Initial weights done! Processing time:{:.10}s'.format(time.time() - start_time))\n",
    "        return all_weights\n",
    "    \n",
    "    def _set_up_training_ops(self, **kwargs):\n",
    "        print('Create graph...')\n",
    "        # Init parameters\n",
    "        dropout_prob = kwargs.get('dropout_prob') if kwargs.get('dropout_prob') else 0.2\n",
    "        transfer_fn = kwargs.get('transfer_fn') if kwargs.get('transfer_fn') else tf.sigmoid # hidden layer\n",
    "        output_fn = kwargs.get('output_fn') if kwargs.get('output_fn') else tf.identity # output layer\n",
    "        n_hidden = kwargs.get('n_hidden') if kwargs.get('n_hidden') else 50\n",
    "        \n",
    "        # AdaGrad Parameters\n",
    "        ada_beta = kwargs.get('ada_beta') if kwargs.get('ada_beta') else 1.0\n",
    "        ada_learning_rate = kwargs.get('ada_learning_rate') if kwargs.get('ada_learning_rate') else 0.01\n",
    "        \n",
    "        optimizer = tf.train.AdagradOptimizer(ada_learning_rate, ada_beta)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(ada_learning_rate)\n",
    "        \n",
    "        # Regularization Parameter\n",
    "        reg_lambda = kwargs.get('reg_lambda') if kwargs.get('reg_lambda') else 1.0\n",
    "        \n",
    "        # Model ops\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_items])\n",
    "        self.z = lambda u: transfer_fn(tf.add_n([tf.matmul(self.y, self.weight['W']),\n",
    "                                                 tf.reshape(self.weight['V'][u], [1, n_hidden]), # (1, 50)\n",
    "                                                 tf.reshape(self.weight['b'], [1, n_hidden])])) # (1, 50)\n",
    "        self.y_hat = lambda u: output_fn(tf.add(tf.matmul(self.z(u), \n",
    "                                                          tf.transpos(self.weight['W_prime'])), # (1, 5551)\n",
    "                                                tf.reshape(self.weight['b_prime'], [1, self.n_items]))) # (1, 16980)\n",
    "        \n",
    "        self.loss = lambda u: tf.nn.l2_loss(tf.subtract(self.y, self.y_hat(u)))\n",
    "        \n",
    "        # Gradient Ops\n",
    "        self.dLoss_d = lambda u, var: optimizer.compute_gradients(self.loss(u), var_list=[var])[0][0]\n",
    "        self.dCost_d = lambda u, var: tf.multiply(1.0 / self.n_users, self.dLoss_d(u, var)) - tf.multiply(reg_lambda, var)\n",
    "        self.cost_gradient_apply = lambda u, var: optimizer.apply_gradients([[self.dCost_d(u, var), var]])\n",
    "        print('Create graph done!')\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def _create_augmented_0_set(self, ith_data, neg_sample_ratio=5):\n",
    "        print('Create augmented data set...')\n",
    "        pos_example_indices = np.where(ith_data > 0)[0]\n",
    "        neg_example_indices = np.where(ith_data == 0)[0]\n",
    "        aug_set = {index for index in pos_example_indices}\n",
    "        if len(neg_example_indices) <= neg_sample_ratio * len(pos_example_indices):\n",
    "            for index in neg_example_indices:\n",
    "                aug_set.add(index)\n",
    "        else:\n",
    "            for index in np.random.permutation(neg_example_indices)[0:neg_sample_ratio * len(pos_example_indices)]:\n",
    "                aug_set.add(index)\n",
    "        print('Create augmented data set done!')\n",
    "        return aug_set    \n",
    "    \n",
    "    def train(self, **kwargs):\n",
    "        MAX_ITERATION = kwargs.get('max_iteration') if kwargs.get('max_iteration') else 1000\n",
    "        dropout_prob = kwargs.get('dropout_prob') if kwargs.get('dropout_prob') else 0.2\n",
    "        n_hidden = kwargs.get('n_hidden') if kwargs.get('n_hidden') else 50\n",
    "        dropout_prob = tf.constant(dropout_prob, dtype=tf.float32)\n",
    "        iteration = 0\n",
    "\n",
    "        self.weight = self._init_variables(n_hidden) \n",
    "        self._set_up_training_ops(**kwargs)\n",
    "        \n",
    "        old_avg_cost = None\n",
    "        model_improved = True\n",
    "        input_data = self.input_data.get_data()\n",
    "        sess = tf.Session()\n",
    "        start_time = time.time()\n",
    "        print('Initialing all Variables...')\n",
    "        sess.run(self.init_op)\n",
    "        print('Done! Process time:{:.10}s'.format(time.time() - start_time))\n",
    "        print('Start Training... ')\n",
    "        \n",
    "        while iteration < MAX_ITERATION or model_improved: \n",
    "            print('=== Iter:{} ==='.format(iteration))\n",
    "            old_avg_cost = avg_cost if iteration >= 1 else None \n",
    "            sum_cost = 0\n",
    "            \n",
    "            for u in range(self.n_users):\n",
    "                val_data = input_data[u].get_full_data() # the data value incloud 0 and 1\n",
    "                train_data = input_data[u].get_train_data()\n",
    "                    \n",
    "                if self.sparse:\n",
    "                    try:\n",
    "                        val_data = sess.run(tf.sparse_tensor_to_dense(val_data))\n",
    "                        train_data = sess.run(tf.sparse_tensor_to_dense(train_data))\n",
    "                    except:\n",
    "                        print('user: {}'.format(u))\n",
    "                # using Equation 9.\n",
    "                y_tilde = sess.run(tf.nn.dropout(tf.constant(train_data, \n",
    "                                                             dtype=tf.float32, \n",
    "                                                             shape=[1, self.n_items]), dropout_prob))\n",
    "                augmented_0_set = self._create_augmented_0_set(np.array(val_data))\n",
    "                    \n",
    "                for item in augmented_0_set:\n",
    "                    sess.run(self.cost_gradient_apply(u, self.weight['W_prime'][item]), feed_dict={self.y: y_tilde})\n",
    "                    sess.run(self.cost_gradient_apply(u, self.weight['b_prime'][item]), feed_dict={self.y: y_tilde})\n",
    "                \n",
    "                for item in np.where(y_tilde == 1):\n",
    "                    sess.run(self.cost_gradient_apply(u, self.weight['W'][item]), feed_dict={self.y: y_tilde})\n",
    "                    \n",
    "                sess.run(self.cost_gradient_apply(u, self.weight['V'][u]), feed_dict={self.y: y_tilde})\n",
    "                sess.run(self.cost_gradient_apply(u, self.weight['b']), feed_dict={self.y: y_tilde})\n",
    "                sum_cost += sess.run(self.loss(u), feed_dict={self.y: val_data})\n",
    "            avg_cost = sum_cost / self.n_users\n",
    "            model_improved = True if old_avg_cost == None or old_avg_cost > avg_cost else False\n",
    "            print('Iteration: {}, Average cost per User:{}'.format(i, cost))\n",
    "            iteration += 1\n",
    "        sess.close()\n",
    "    \n",
    "    def _count_num_accurate_classification(self, ground_turth, recommendations):\n",
    "        ''' \n",
    "        |Intersection(C-adopted, C-NRecommended)|\n",
    "        \n",
    "        :param List ground_turth  : Actual items that user liked\n",
    "        :param List recommendation: Items the system recommended to the user\n",
    "        \n",
    "        :return Int               : Length of intersection between system recommended items and \n",
    "                                    items the user actually liked \n",
    "        '''\n",
    "        num_accurate_predictions = 0\n",
    "        for item in recommendations:\n",
    "            if item in ground_turth:\n",
    "                num_accurate_predictions += 1\n",
    "        return num_accurate_predictions        \n",
    "    \n",
    "    def precision(self, actual, predicted, N=5):\n",
    "        ''' \n",
    "        Calculates precision defined by: |Intersection(C-adopted, C-NRecommended)| / N\n",
    "        \n",
    "        :param List actual   : Actual items that were liked\n",
    "        :param List predicted: Prediction of actual items\n",
    "        :param Int  N        : Number of items to be recommended (default=5)\n",
    "\n",
    "        :return Float        : Number b/w 0 to 1 corresponding to the precision of the recommendation list\n",
    "        '''\n",
    "        if len(predicted) > N:\n",
    "            predicted = predicted[:N]\n",
    "        num_accurate_predictions = self._count_num_accurate_classification(actual, predicted)\n",
    "        return float(num_accurate_predictions) / N    \n",
    "            \n",
    "    def recall(self, actual, predicted, N=5):\n",
    "        ''' \n",
    "        Calculates recall defined by: |Intersection(C-adopted, C-NRecommended)| / C-adopted\n",
    "        \n",
    "        :param List actual   : Actual items that were liked\n",
    "        :param List predicted: Prediction of actual items, \n",
    "        :param Int  N        : Number of items to be recommended (default=5)\n",
    "\n",
    "        :return Float        : Number b/w 0 to 1 corresponding to the recall of the recommendation list\n",
    "        '''\n",
    "        num_accurate_predictions = self._count_num_accurate_classification(actual, predicted)\n",
    "        return float(num_accurate_predictions) / len(actual)\n",
    "        \n",
    "    \n",
    "    def recommend(self, u, Y, N=5, test_set=False):\n",
    "        '''\n",
    "        Method recommend outputs the top-N item recommendations for user u\n",
    "           \n",
    "        :param Int u                : Id for user to be recommended\n",
    "        :param AbstractInputSource Y: User data class that encapsulates training, test, and full data\n",
    "        :param Int N                : Number of items to be recommended\n",
    "        :param Boolean test_set     : Necessary flag for checking the accuracy of the training set to the test set (defualt=Fasle)\n",
    "\n",
    "        :return List                : List: Top N list of items recommend to user u  \n",
    "        '''        \n",
    "        \n",
    "        if test_set:\n",
    "            recommendation_candidates = Y.get_neg_indices()\n",
    "            \n",
    "            if self.sparse:\n",
    "                train_data = self.sess.run(tf.sparse_tensor_to_dense(Y.get_train_data()))\n",
    "            else:\n",
    "                train_data = Y.get_train_data()\n",
    "            feed_dict = {self.y: train_data}\n",
    "        else:\n",
    "            recommendation_candidates = np.where(Y==0)\n",
    "            feed_dict = {self.y: Y}\n",
    "        y_hat = self.sess.run(self.y_hat(u), feed_dict=feed_dict)\n",
    "        top_items_sorted = sorted(recommendation_candidates, key=lambda i: y_hat[i], reverse=True)\n",
    "        \n",
    "        return top_items_sorted[:N] if len(top_items_sorted) > N else top_items_sorted\n",
    "        \n",
    "    def average_precision(self, actual, u, Y, N):\n",
    "        '''\n",
    "        Average Precision at N is defined as: sum(Precision@k * Rel(K), k, 1, N) / min(N, |C-adopted|)\n",
    "        \n",
    "        :param List actual   : Actual items that were liked\n",
    "        :param Int u                : Represents user u \n",
    "        :param AbstractInputSource Y: User data class that encapsulates training, test, and full data\n",
    "        :param Int N                : Number of items to be recommended\n",
    "\n",
    "        :return Float               : Number b/w 0 to 1 corresponding to the Average precision of the recommendation list\n",
    "        '''\n",
    "        min_N_adopted = min(N, len(actual))\n",
    "        ground_turth = Y.get_test_data()\n",
    "        predicted = self.recommend(u, Y, N=N, test_set=True)\n",
    "        \n",
    "        sum_precision = 0\n",
    "        for k in range(1, N+1):\n",
    "            if predicted[k] in ground_turth:\n",
    "                sum_precision += self.precision(ground_turth, predicted, N=k)\n",
    "            else:\n",
    "                sum_precision += 0.0\n",
    "                \n",
    "        return sum_precision / min_N_adopted\n",
    "\n",
    "    def mean_average_precision(self, N):\n",
    "        '''\n",
    "        Average of AP for all users\n",
    "        \n",
    "        :param Int N : Number of items to be recommended\n",
    "        \n",
    "        :return Float: Number b/w 0 to 1 corresponding to the Mean Average Precision@N\n",
    "        '''\n",
    "        \n",
    "        sum_ap = 0\n",
    "        for i, user in enumerate(self.input_data.get_data()):\n",
    "            sum_ap += self.average_precision(i, user, N)\n",
    "            \n",
    "        return sum_ap / len(self.users)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdae_model = CDAE(CiteYouLikeA, sparse=False)\n",
    "cdae_model = CDAE(input_data)\n",
    "cdae_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SavedModel module to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_path = Path(HOME) / 'tensorboard_files'\n",
    "ckpt_dir = str(HOME / 'tensorflow_ckpt' / 'DeepRcommenderOneAD')\n",
    "export_dir = os.path.join(ckpt_dir, 'export')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "with tf.Graph().as_default() as g: \n",
    "    valid_dataset = load_data(files_list=validation_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                              batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=0) \n",
    "    iterator = tf.data.Iterator.from_structure(valid_dataset().output_types, \n",
    "                                               valid_dataset().output_shapes)    \n",
    "    batch_index, batch_features = iterator.get_next()\n",
    "    valid_init_op = iterator.make_initializer(valid_dataset())    \n",
    "\n",
    "    \n",
    "with tf.Session(config=config, graph=g) as pred_sess:\n",
    "    total_valid_steps = 99999\n",
    "    meta_graph_def = tf.saved_model.loader.load(pred_sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
    "    signature = meta_graph_def.signature_def\n",
    "    x_tensor_name = signature[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs['inputs'].name\n",
    "    y_tensor_name = signature[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs['outputs'].name\n",
    "\n",
    "    #graph = tf.get_default_graph()\n",
    "    #graph_nodes_name = [n.name for n in graph.as_graph_def().node]\n",
    "    pred_sess.run(valid_init_op)\n",
    "    prediction_recoder = []\n",
    "    X = pred_sess.graph.get_tensor_by_name(x_tensor_name)\n",
    "    Y = pred_sess.graph.get_tensor_by_name(y_tensor_name)\n",
    "    for _ in range(total_valid_steps):\n",
    "        try:\n",
    "            valid_labels, valid_features = pred_sess.run([batch_index, batch_features])\n",
    "            decode_valid_labels = [l.decode('utf-8') for l in valid_labels]            \n",
    "            predicted_result = pred_sess.run(Y, feed_dict={X: valid_features})                         \n",
    "            prediction_recoder.append(list(zip(decode_valid_labels, predicted_result)))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "print('Num of iteration:{}, total predicted count: {}'.format(len(prediction_recoder), \n",
    "                                                              len(prediction_recoder) * NUM_BATCH_SIZE))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "def get_remaind_list(_data, _target_uuid_list):\n",
    "    remained_uuid_list = []\n",
    "    for k, v in _data:\n",
    "        if k in _target_uuid_list:\n",
    "            remained_uuid_list.append((k, v))\n",
    "    # [(k, v) for k, v in _data if k in _target_uuid_list]            \n",
    "    return remained_uuid_list\n",
    "\n",
    "target_uuid_list = test_data.parse_uuid.values.tolist()\n",
    "pool_size = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=pool_size)\n",
    "result = []\n",
    "for d in prediction_recoder:\n",
    "    result.append(pool.apply_async(get_remaind_list, args=(d, target_uuid_list, )))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "get_process_result = []\n",
    "for res in result:\n",
    "    get_process_result.append(res.get(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 5 # len(data_schema['DailyVideoQueue'])\n",
    "has_recommend_list = []\n",
    "for i, line in enumerate(get_process_result):\n",
    "    for uuid, pred_matrix in line:\n",
    "        #actual_data = test_data[test_data.parse_uuid == uuid].iloc[:, :-1].values[0]\n",
    "        recommendation_candidates  = np.where(pred_matrix > 0)[0]\n",
    "        top_items_sorted = sorted(recommendation_candidates, key=lambda i: pred_matrix[i], reverse=True)\n",
    "        top_items_socre = [pred_matrix[i] for i in top_items_sorted]\n",
    "        top_recommended_items = top_items_sorted[:N] if len(top_items_sorted) > N else top_items_sorted\n",
    "        recommended_items_name = [data_schema['DataSchema'][i] for i in top_recommended_items]\n",
    "        #recommend_video_list = [v for v in recommended_items_name if int(v) in data_schema['DailyVideoQueue']]\n",
    "        recommend_video_list = [v for v in recommended_items_name if v in data_schema['DataSchema']]\n",
    "        if len(recommend_video_list) != 0:\n",
    "            has_recommend_list.append((uuid, recommend_video_list, top_items_socre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_uuid_list = [uuid for uuid, _, _ in has_recommend_list]\n",
    "target_uuid_list = test_data.parse_uuid.values.tolist()\n",
    "target_uuid_dict = {}\n",
    "for k, v in enumerate(target_uuid_list):\n",
    "    target_uuid_dict[v] = k\n",
    "    \n",
    "remaind_index_list = [target_uuid_dict[uuid] for uuid in pred_uuid_list]\n",
    "#remaind_index_list = [target_uuid_list.index(uuid) for i, uuid in enumerate(pred_uuid_list) if uuid in target_uuid_list]\n",
    "print('Remained numnber of index: {}'.format(len(remaind_index_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g, config=config) as sess:\n",
    "    random_user_set = np.random.permutation(N_USERS)\n",
    "    old_avg_cost = None\n",
    "    model_improved = True\n",
    "    epoch = 0\n",
    "    total_train_steps = math.floor(N_USERS / NUM_BATCH_SIZE)\n",
    "    \n",
    "    # Best validation loss seen so far.\n",
    "    best_valid_loss = 99999\n",
    "        \n",
    "    # Iteration-number for last improvement to validation loss.\n",
    "    last_improvement = 0\n",
    "\n",
    "    # Stop optimization if no improvement found in this many iterations.\n",
    "    require_improvement = 2\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_train_path, graph=sess.graph,\n",
    "                                         filename_suffix='CDAE')\n",
    "    test_writer = tf.summary.FileWriter(tensorboard_test_path, graph=sess.graph,\n",
    "                                        filename_suffix='CDAE')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    start_time = time.time()\n",
    "    print('Initialing all Variables...')\n",
    "    sess.run(init)\n",
    "    print('Initialing all variables done! Process time:{:.10}s'.format(time.time() - start_time))\n",
    "    print('Start Training... ')           \n",
    "        \n",
    "    while epoch <= NUM_EPOCHS:\n",
    "        print('=== Epoch: {}, Total batch iter:{} ==='.format(epoch, total_train_steps))\n",
    "        sum_cost = 0\n",
    "        start_time = time.time()\n",
    "        for i, mini_batch in enumerate(iterate_one_epoch(train_data, NUM_BATCH_SIZE, N_ITEMS)):            \n",
    "            mask_corruption = np.random.binomial(1, 1 - CORRUPTION_LEVEL, (NUM_BATCH_SIZE, N_ITEMS))\n",
    "            mask_corruption_np = np.array(mask_corruption, dtype=np.float32) \n",
    "            maskt_augmented_O = create_augmented_O_set(mini_batch['data'])\n",
    "            \n",
    "            train_loss, _ = sess.run([tmp_loss, cost_gradient_apply], \n",
    "                                     feed_dict={Y: mini_batch['data'], \n",
    "                                                corrupted_mask: np_mask_corruption,\n",
    "                                                input_ids: mini_batch['index'],\n",
    "                                                augmented_O_mask: maskt_augmented_O})\n",
    "            sum_cost += train_loss\n",
    "            if i % 50 == 0 and i > 0:\n",
    "                print('Iteration: {:4d}, loss: {:.8f}'.format(i, train_loss))\n",
    "                summary = sess.run(merged_summary, feed_dict={Y: mini_batch['data'], \n",
    "                                                              corrupted_mask: np_mask_corruption,\n",
    "                                                              input_ids: mini_batch['index'],\n",
    "                                                              augmented_O_mask: maskt_augmented_O})    \n",
    "                train_writer.add_summary(summary, epoch * total_train_steps + i)\n",
    "                if np.isnan(train_loss):\n",
    "                    break        \n",
    "            \n",
    "        end_time = time.time()\n",
    "        avg_cost = sum_cost / i\n",
    "        \n",
    "        # Validation\n",
    "        for i, mini_batch in enumerate(iterate_one_epoch_valid(train_data, NUM_BATCH_SIZE, N_ITEMS)):            \n",
    "            mask_corruption = np.random.binomial(1, 1 - CORRUPTION_LEVEL, (NUM_BATCH_SIZE, N_ITEMS))\n",
    "            mask_corruption_np = np.array(mask_corruption, dtype=np.float32) \n",
    "            maskt_augmented_O = create_augmented_O_set(mini_batch['data'])\n",
    "            \n",
    "            train_loss, _ = sess.run([tmp_loss, cost_gradient_apply], \n",
    "                                     feed_dict={Y: mini_batch['data'], \n",
    "                                                corrupted_mask: np_mask_corruption,\n",
    "                                                input_ids: mini_batch['index'],\n",
    "                                                augmented_O_mask: maskt_augmented_O})\n",
    "            sum_cost += train_loss\n",
    "            if i % 50 == 0 and i > 0:\n",
    "                print('Iteration: {:4d}, loss: {:.8f}'.format(i, train_loss))\n",
    "                summary = sess.run(merged_summary, feed_dict={Y: mini_batch['data'], \n",
    "                                                              corrupted_mask: np_mask_corruption,\n",
    "                                                              input_ids: mini_batch['index'],\n",
    "                                                              augmented_O_mask: maskt_augmented_O})    \n",
    "                train_writer.add_summary(summary, epoch * total_train_steps + i)\n",
    "                if np.isnan(train_loss):\n",
    "                    break        \n",
    "            \n",
    "        end_time = time.time()\n",
    "        avg_cost = sum_cost / i\n",
    "        \n",
    "        # If validation accuracy is an improvement over best-known.\n",
    "        if avg_cost < best_valid_loss:\n",
    "            best_valid_loss = avg_cost                            \n",
    "            last_improvement = epoch\n",
    "            saver.save(sess, os.path.join(ckpt_dir, 'DeepRecommender.ckpt'), \n",
    "                       global_step=epoch * total_train_steps + i)\n",
    "            improved_str = '*'        \n",
    "\n",
    "        if epoch - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization.\")\n",
    "            break\n",
    "            \n",
    "        #model_improved = True if old_avg_cost == None or old_avg_cost > avg_cost else False       \n",
    "        print('#### Epoch: {}, Aerage Cost per User:{:.10f}, Processing time:{:.5f}s'.format(epoch, \n",
    "                                                                                             avg_cost, \n",
    "                                                                                             end_time - start_time))\n",
    "        epoch += 1                 \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
