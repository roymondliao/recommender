{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from tensorflow.python.keras import backend as K\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.core.protobuf import config_pb2\n",
    "from scipy.sparse import csr_matrix\n",
    "from tensorflow.python import debug as tfdbg\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBEL_DEVICES'] = '0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path(os.environ['HOME'])\n",
    "training_path = HOME / 'ninja_project' / 'data' / 'netflix' / 'training'\n",
    "src_files = [str(training_path / f) for f in os.listdir(str(training_path)) if 'n3m' in f]\n",
    "\n",
    "tensorboard_path = Path(HOME) / 'tensorboard_files'\n",
    "ckpt_dir = str(HOME / 'tensorflow_ckpt' / 'DeepRcommenderOneAD')\n",
    "\n",
    "tensorboard_train_path = str(tensorboard_path / 'adr_train_onead')\n",
    "tensorboard_test_path = str(tensorboard_path / 'adr_test_onead')\n",
    "for path in [tensorboard_train_path, tensorboard_test_path, ckpt_dir]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "            \n",
    "for path in [tensorboard_train_path, tensorboard_test_path, ckpt_dir]:\n",
    "    if len(os.listdir(path)) > 0:\n",
    "        for f in os.listdir(path):\n",
    "            if os.path.isdir(os.path.join(path, f)):\n",
    "                shutil.rmtree(os.path.join(path, f))\n",
    "            else:\n",
    "                os.remove(os.path.join(path, f))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load video data & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 10, 0, 58, 22, 65091)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = datetime.now()\n",
    "bucket = \"<GCP bucket>\"\n",
    "bucket_prefix = '<training data path>'\n",
    "def check_date_in_storage(_date, bucket_name, bucket_prefix):\n",
    "    \n",
    "    def storage_newest_path(_bucket_name, _prefix_path):\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(_bucket_name)\n",
    "        return isinstance(bucket.get_blob(_prefix_path), blob.Blob)\n",
    "    \n",
    "    while True:\n",
    "        month = datetime.strftime(_date.date(), \"%m\")\n",
    "        day = datetime.strftime(_date.date(), \"%d\")\n",
    "        prefix_path = os.path.join(bucket_prefix,\n",
    "                                   'year={0}/month={1}/day={2}/'.format(_date.year, month, day))\n",
    "        if storage_newest_path(bucket_name, prefix_path):\n",
    "            break\n",
    "        else:\n",
    "            _date = _date - timedelta(days=1)\n",
    "    return _date\n",
    "\n",
    "get_check_date = check_date_in_storage(current_date, bucket, bucket_prefix)\n",
    "get_check_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data in GCP\n",
    "training_data_perfix = os.path.join('training data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "\n",
    "validation_data_perfix = os.path.join('validation data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "testing_data_perfix = os.path.join('testing data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "click_testing_data_perfix = os.path.join('click testing data path/year={}/month={}/day={}/'.format(\n",
    "    get_check_date.year,\n",
    "    datetime.strftime(get_check_date.date(), \"%m\"),\n",
    "    datetime.strftime(get_check_date.date(), \"%d\")))\n",
    "\n",
    "training_files = tf.gfile.ListDirectory(training_data_perfix)\n",
    "validation_files = tf.gfile.ListDirectory(validation_data_perfix)\n",
    "testing_files = tf.gfile.ListDirectory(testing_data_perfix)\n",
    "click_testing_files = tf.gfile.ListDirectory(click_testing_data_perfix)\n",
    "\n",
    "training_list = [os.path.join(training_data_perfix, f) for f in training_files if '.csv' in f]\n",
    "validation_list = [os.path.join(validation_data_perfix, f) for f in validation_files if '.csv' in f]\n",
    "testing_list = [os.path.join(testing_data_perfix, f) for f in testing_files if '.csv' in f]\n",
    "click_testing_list = [os.path.join(click_testing_data_perfix, f) for f in click_testing_files if '.csv' in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data schema\n",
    "video_data_info_perfix = os.path.join('gs://onedata-prod/event/serve/adr/')\n",
    "video_data_info_files = tf.gfile.ListDirectory(video_data_info_perfix)\n",
    "video_data_info_list = [os.path.join(video_data_info_perfix, f) for f in video_data_info_files if '.json' in f]\n",
    "\n",
    "with file_io.FileIO(video_data_info_list[0], mode='r') as input_f:\n",
    "    data_schema = json.load(input_f)\n",
    "\n",
    "\n",
    "if data_schema['CreateDate'] != get_check_date.strftime('%Y-%m-%d'):\n",
    "    sys.exit(ValueError)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_data = pd.DataFrame()\n",
    "for file in training_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        raw_data = pd.concat([raw_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.DataFrame()\n",
    "for file in validation_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        valid_data = pd.concat([valid_data, res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame()\n",
    "for file in testing_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        test_data = pd.concat([test_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((474632, 85), (1110843, 85))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_test_data = pd.DataFrame()\n",
    "for file in click_testing_list:\n",
    "    with file_io.FileIO(file, mode='r') as input_f:\n",
    "        res = pd.read_csv(input_f)\n",
    "        click_test_data = pd.concat([click_test_data, res], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5454, 85)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files_list, features_name, batch_size, epochs, is_training, name):\n",
    "    def _parse_function(_line, _name):\n",
    "        num_features = len(features_name)\n",
    "        num_columns = num_features + 1\n",
    "        record_defaults = [[]] * (num_features) + [['string']]\n",
    "        decode_line = tf.decode_csv(_line, record_defaults=record_defaults, field_delim=',', name=_name)        \n",
    "        #features = dict(zip(num_features, decode_line[:num_features]))\n",
    "        index = tf.cast(decode_line[-1], tf.string)\n",
    "        features = tf.reshape(decode_line[:num_features], shape=[num_features])\n",
    "        return index, features\n",
    "\n",
    "    def _input_fn():\n",
    "        #match_pattern = tf.placeholder(dtype=tf.string)\n",
    "        #filenames = tf.train.match_filenames_once(pattern=match_pattern, name=None)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(files_list)\n",
    "        dataset = dataset.flat_map(lambda filename: (tf.data.TextLineDataset(filename).\\\n",
    "                                                     skip(1).\\\n",
    "                                                     map(lambda line: _parse_function(line, name), \n",
    "                                                         num_parallel_calls=multiprocessing.cpu_count())))\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(buffer_size=256)\n",
    "            #dataset = dataset.repeat(epochs)    \n",
    "        dataset = dataset.prefetch(buffer_size=batch_size * 100) # n = 元素個數 / Batch size \n",
    "        dataset = dataset.batch(batch_size)                                            \n",
    "        return dataset\n",
    "    return _input_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "'''\n",
    "# reset graph and setting sess config\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Graph().as_default() as g:    \n",
    "    with g.device('/cpu:0'):\n",
    "        with tf.name_scope('DataPipelines'):\n",
    "            train_dataset = load_data(files_list=training_list, features_name=data_schema['DataSchema'][:-1],\n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=1, name='train')    \n",
    "            iterator = tf.data.Iterator.from_structure(train_dataset().output_types, \n",
    "                                                       train_dataset().output_shapes)    \n",
    "            batch_index, batch_features = iterator.get_next()\n",
    "            train_init_op = iterator.make_initializer(train_dataset())            \n",
    "            \n",
    "with tf.Session(graph=g, config=config) as sess:\n",
    "    total_train_steps = 99999\n",
    "    for epoch in range(1):        \n",
    "        tic = time.time()\n",
    "        sess.run(train_init_op)\n",
    "        for i in range(total_train_steps):                \n",
    "            try:\n",
    "                train_labels, train_features = sess.run([batch_index, batch_features])                \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Finished, time:{}'.format(time.time() - tic))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropBlock\n",
    "class DropBlock(tf.keras.layers.Layer) :\n",
    "    def __init__(self, keep_prob, block_size, **kwargs):\n",
    "        super(DropBlock, self).__init__(**kwargs)\n",
    "        self.keep_prob = float(keep_prob) if isinstance(keep_prob, int) else keep_prob\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        _, self.h, self.w, self.channel = input_shape.as_list()\n",
    "        # pad the mask\n",
    "        bottom = right = (self.block_size -1) // 2\n",
    "        top = left = (self.block_size -1) - bottom\n",
    "        self.padding = [[0, 0], [top, bottom], [left, right], [0, 0]]\n",
    "        self.set_keep_prob()\n",
    "        super(DropBlock, self).build(input_shape)\n",
    "        \n",
    "    def set_keep_prob(self, keep_prob=None):\n",
    "        \"\"\"This method only support Eager Execution\"\"\"\n",
    "        if keep_prob is not None:\n",
    "            self.keep_prob = keep_prob\n",
    "        w, h = tf.to_float(self.w), tf.to_float(self.h)\n",
    "        self.gamma = (1. - self.keep_prob) * (w * h) / (self.block_size ** 2) / ((w - self.block_size + 1) * (h - self.block_size + 1))\n",
    "\n",
    "    def _create_mask(self, input_shape):\n",
    "        sampling_mask_shape = tf.stack([input_shape[0], \n",
    "                                        self.h - self.block_size + 1, \n",
    "                                        self.w - self.block_size + 1,\n",
    "                                        self.channel])\n",
    "        mask = DropBlock._bernoulli(sampling_mask_shape, self.gamma)\n",
    "        # 擴充行列，並給予0值，依據 paddings 參數給予的上下左右值來做擴充，mode有三種模式可選，可參考 document\n",
    "        mask = tf.pad(tensor=mask, paddings=self.padding, mode='CONSTANT') \n",
    "        mask = tf.nn.max_pool(value=mask, \n",
    "                              ksize=[1, self.block_size, self.block_size, 1], \n",
    "                              strides=[1, 1, 1, 1], \n",
    "                              padding='SAME')\n",
    "        mask = 1 - mask\n",
    "        return mask\n",
    "        \n",
    "    @staticmethod    \n",
    "    def _bernoulli(shape, mean):\n",
    "        return tf.nn.relu(tf.sign(mean - tf.random_uniform(shape, minval=0, maxval=1, dtype=tf.float32)))\n",
    "    \n",
    "    # The call function is a built-in function in 'tf.kera'.\n",
    "    def call(self, inputs, training=None, scale=True, **kwargs):\n",
    "        def drop():\n",
    "            mask = self._create_mask(tf.shape(inputs))\n",
    "            output = inputs * mask\n",
    "            output = tf.cond(tf.constant(scale, dtype=tf.bool) if isinstance(scale, bool) else scale,\n",
    "                             true_fn=lambda: output * tf.to_float(tf.size(mask)) / tf.reduce_sum(mask),\n",
    "                             false_fn=lambda: output)\n",
    "            return output\n",
    "        \n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "        output = tf.cond(tf.logical_or(tf.logical_not(training), tf.equal(self.keep_prob, 1.0)),\n",
    "                         true_fn=lambda: inputs,\n",
    "                         false_fn=drop)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5, 3)\n",
      "[[1.2711865 1.2711865 1.2711865 1.2711865 1.2711865]\n",
      " [1.2711865 1.2711865 1.2711865 1.2711865 1.2711865]\n",
      " [1.2711865 1.2711865 1.2711865 1.2711865 1.2711865]\n",
      " [1.2711865 1.2711865 1.2711865 1.2711865 1.2711865]\n",
      " [1.2711865 1.2711865 1.2711865 1.2711865 1.2711865]]\n",
      "[[0 1 0 0 1]\n",
      " [0 1 0 1 1]\n",
      " [0 1 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32, [None, 5, 5, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "drop_block = DropBlock(keep_prob=keep_prob, block_size=4)\n",
    "b = drop_block(inputs=a, training=training)\n",
    "noise_dist = tf.distributions.Bernoulli(probs=[0.5])\n",
    "mask = noise_dist.sample(tf.stack(tf.shape(a)))\n",
    "    \n",
    "sess = tf.Session()\n",
    "feed_dict = {a: np.ones([2, 5, 5, 3]), keep_prob: 0.8, training: True}\n",
    "c, c_mask = sess.run([b, mask], feed_dict=feed_dict)\n",
    "print(c.shape)\n",
    "print(c[0, :, :, 0])\n",
    "print(c_mask.reshape([2, 5, 5, 3])[0, :, :, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x, fn, name=None):\n",
    "    if fn == 'selu':\n",
    "        return tf.nn.selu(x, name)\n",
    "    elif fn == 'relu':\n",
    "        return tf.nn.relu(x, name)\n",
    "    elif fn == 'sigmoid':\n",
    "        return tf.nn.sigmoid(x, name)\n",
    "    elif fn == 'relu6':\n",
    "        return tf.nn.relu6(x, name)\n",
    "    elif fn == 'elu':\n",
    "        return tf.nn.elu(x, name)\n",
    "    elif fn == 'lrelu':\n",
    "        return tf.nn.leaky_relu(x, 0.2, name)\n",
    "    elif fn == None:\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linearity type')\n",
    "\n",
    "def initialize_weights(layers, is_constrained=False):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    with tf.variable_scope('hyperparameters', reuse=tf.AUTO_REUSE):\n",
    "        with tf.name_scope('Weights'):\n",
    "            init = tf.contrib.layers.xavier_initializer(seed=202109)\n",
    "            for i in range(len(layers) - 1):\n",
    "                layer_name = 'layer%s' % i\n",
    "                W[i + 1] = tf.get_variable(name=\"W\" + str(i + 1), shape=(layers[i], layers[i + 1]), initializer=init)            \n",
    "                if is_constrained:\n",
    "                    W[2 * len(layers) - 2 - i] = tf.transpose(W[i + 1])\n",
    "                else:            \n",
    "                    W[2 * len(layers) - 2 - i] = tf.get_variable(name=\"W\" + str(2 * len(layers) - 2 - i),\n",
    "                                                                 shape=(layers[i + 1], layers[i]),\n",
    "                                                                 initializer=init)\n",
    "                tf.summary.histogram(name=layer_name + '/Weights', values=W[i+1])\n",
    "                tf.summary.histogram(name=layer_name + '/Weights', values=W[2 * len(layers) - 2 - i])\n",
    "            \n",
    "        with tf.name_scope('Biases'):\n",
    "            for i in range(len(layers) - 1):\n",
    "                layer_name = 'layer%s' % i    \n",
    "                b[i + 1] = tf.get_variable(name=\"b\" + str(i + 1),\n",
    "                                           shape=(layers[i + 1], ),\n",
    "                                           initializer=tf.zeros_initializer())\n",
    "                b[2 * len(layers) - 2 - i] = tf.get_variable(name=\"b\" + str(2 * len(layers) - 2 - i),\n",
    "                                                             shape=(layers[i], ),\n",
    "                                                             initializer=tf.zeros_initializer())\n",
    "                tf.summary.histogram(name=layer_name + '/Biases', values=b[i + 1])\n",
    "                tf.summary.histogram(name=layer_name + '/Biases', values=b[2 * len(layers) - 2 - i])\n",
    "            \n",
    "    return W, b\n",
    "\n",
    "def encoder(x, weights, func, drop_prob):\n",
    "    W, b = weights\n",
    "    layers = int(len(W) / 2)\n",
    "    with tf.name_scope('Encoder'):\n",
    "        for layer in range(1, layers + 1):\n",
    "            if isinstance(x, tf.SparseTensor):\n",
    "                x = activation(tf.nn.bias_add(tf.sparse_tensor_dense_matmul(sp_a=x, b=W[layer]), b[layer]), func)\n",
    "            else:\n",
    "                x = activation(tf.nn.bias_add(tf.matmul(x, W[layer]), b[layer]), func)\n",
    "            if drop_prob:\n",
    "                x = tf.nn.dropout(x, drop_prob)\n",
    "    return x\n",
    "\n",
    "def decoder(x, weights, func, last_func):\n",
    "    W, b = weights\n",
    "    layers = int(len(W) / 2)\n",
    "    with tf.name_scope('Decoder'):\n",
    "        for layer in range(layers + 1, 2 * layers):\n",
    "            x =  activation(tf.nn.bias_add(tf.matmul(x, W[layer]), b[layer]), func)\n",
    "        # output layer    \n",
    "        x = activation(tf.nn.bias_add(tf.matmul(x, W[len(W)]), b[len(b)]), last_func, name='Prediction')\n",
    "    return x\n",
    "\n",
    "def autoencoder(x, layers, is_constrained=False, drop_prob=1.0, func='selu', last_func='selu'):\n",
    "    weights = initialize_weights(layers, is_constrained)    \n",
    "    forward = encoder(x, weights, func, drop_prob)\n",
    "    forward = decoder(forward, weights, func, last_func)\n",
    "    return forward, weights\n",
    "\n",
    "def get_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    use Masked Mean Squared Error Loss\n",
    "    \"\"\"\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    mask = tf.not_equal(y, zero)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mse_loss = tf.reduce_sum(tf.multiply(tf.square(y - y_hat), mask))\n",
    "    num_mask = tf.reduce_sum(mask)        \n",
    "    return mse_loss, tf.divide(mse_loss, num_mask)\n",
    "\n",
    "def get_test_loss(y, y_hat):\n",
    "    zero = tf.constant(0, dtype=tf.float32)\n",
    "    mask = tf.not_equal(y, zero)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    y_hatm = tf.multiply(y_hat, mask)\n",
    "    loss = tf.reduce_sum(tf.square(y_hatm - y))\n",
    "    mask = tf.reduce_sum(mask)\n",
    "    return loss, mask\n",
    "\n",
    "def get_optimizer(optimizer_fn, lr, momentum_value):\n",
    "    if optimizer_fn == 'momentum':\n",
    "        return tf.train.MomentumOptimizer(lr, momentum_value)\n",
    "    elif optimizer_fn =='adam':\n",
    "        return tf.train.AdamOptimizer(lr)\n",
    "    else:\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_steps = 1    \n",
    "dense_refeeding = True\n",
    "NUM_EPOCHS = 1\n",
    "NUM_BATCH_SIZE = 128\n",
    "N_ITEMS = len(data_schema['DataSchema'][:-1])\n",
    "#N_USERS = len(user_id_map)\n",
    "LAYERS_SIZE = [N_ITEMS, 128, 256, 256]\n",
    "START_LEARNING_RATE = 0.001\n",
    "MOMENTUM_VALUE = 0.9\n",
    "DROPOUT_RATE = 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Saving model use SavedModel module  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "=== Epoch:  0 ===\n",
      "Iter: 1000, RMSE: 1.440645\n",
      "Iter: 2000, RMSE: 1.242693\n",
      "Iter: 3000, RMSE: 1.123144\n",
      "Iter: 4000, RMSE: 1.043585\n",
      "Iter: 5000, RMSE: 0.984786\n",
      "Iter: 6000, RMSE: 0.938379\n",
      "Iter: 7000, RMSE: 0.900580\n",
      "Iter: 8000, RMSE: 0.868793\n",
      "Iter: 8862 is end of line.\n",
      "Epoch:  0, Train Loss: 0.84492158, Valid Loss:2.07888396, Compute time: 260.99773622\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'/home/yuyuliao/tensorflow_ckpt/DeepRcommenderOneAD/export/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "# reset graph and setting sess config\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Graph().as_default() as g:    \n",
    "    tf.set_random_seed(202109)\n",
    "    with g.device('/cpu:0'):\n",
    "        with tf.name_scope('DataPipelines'):\n",
    "            train_dataset = load_data(files_list=training_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=1, name='train')    \n",
    "            valid_dataset = load_data(files_list=validation_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                                      batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=0, name='valid') \n",
    "            iterator = tf.data.Iterator.from_structure(train_dataset().output_types, \n",
    "                                                       train_dataset().output_shapes)    \n",
    "            batch_index, batch_features = iterator.get_next()\n",
    "            train_init_op = iterator.make_initializer(train_dataset())\n",
    "            valid_init_op = iterator.make_initializer(valid_dataset())\n",
    "            \n",
    "    with g.device('/gpu:0'):        \n",
    "        with tf.name_scope('Placeholder'):\n",
    "            # X = tf.sparse_placeholder(tf.float32, [None, N_ITEMS], name='X')\n",
    "            X = tf.placeholder(tf.float32, [None, N_ITEMS], name='Inputs')\n",
    "            Y = tf.placeholder(tf.float32, [None, N_ITEMS], name='Targets')\n",
    "            global_step = tf.Variable(tf.constant(0), trainable=False)\n",
    "    \n",
    "        with tf.name_scope('Autoencoder'):\n",
    "            Y_hat, weights_dict = autoencoder(x=X, layers=LAYERS_SIZE, is_constrained=False, \n",
    "                                              drop_prob=0.5, func='selu', last_func='selu')\n",
    "        \n",
    "        with tf.name_scope('Loss'):\n",
    "            loss, mmse_loss = get_loss(y=Y, y_hat=Y_hat)\n",
    "            loss_sum, mask_sum = get_test_loss(y=Y, y_hat=Y_hat)\n",
    "            tf.summary.scalar(name='MSELoss', tensor=loss)\n",
    "            tf.summary.scalar(name='MMSELoss', tensor=mmse_loss)\n",
    "            #tf.summary.scalar(name='LossSum', tensor=loss_sum)\n",
    "            #tf.summary.scalar(name='MaskSum', tensor=mask_sum)\n",
    "            \n",
    "    with g.device('/gpu:1'):        \n",
    "        with tf.name_scope('Optimizer'):\n",
    "            #learning_rate = tf.train.exponential_decay(START_LEARNING_RATE, global_step,\n",
    "            #                                           1000, 0.96, staircase=False)\n",
    "            optimizer = get_optimizer('momentum', START_LEARNING_RATE, MOMENTUM_VALUE)\n",
    "            train_step = optimizer.minimize(mmse_loss, global_step=global_step)\n",
    "    #saver = tf.train.Saver()  \n",
    "    \n",
    "\n",
    "with tf.Session(graph=g, config=config) as sess:\n",
    "    \n",
    "    export_dir = os.path.join(ckpt_dir, 'export')        \n",
    "    total_train_steps = 99999\n",
    "    total_valid_steps = 99999\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(tensorboard_train_path, graph=sess.graph, filename_suffix='DeepRecommender')\n",
    "    test_writer = tf.summary.FileWriter(tensorboard_test_path, filename_suffix='DeepRecommender')\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Best validation loss seen so far.\n",
    "    best_valid_loss = 99999\n",
    "        \n",
    "    # Iteration-number for last improvement to validation loss.\n",
    "    last_improvement = 0\n",
    "\n",
    "    # Stop optimization if no improvement found in this many iterations.\n",
    "    require_improvement = 5\n",
    "    \n",
    "    print('Start training...')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print('=== Epoch: {:2d} ==='.format(epoch))\n",
    "        tic = time.time()\n",
    "        total_loss_sum = 0\n",
    "        total_mask_sum = 0\n",
    "        valid_total_loss_sum = 0\n",
    "        valid_total_mask_sum = 0  \n",
    "        train_loss_list = []\n",
    "        train_mmse_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        valid_mmse_loss_list = []\n",
    "        \n",
    "        sess.run(train_init_op)\n",
    "        for i in range(total_train_steps):                \n",
    "            try:\n",
    "                train_labels, train_features = sess.run([batch_index, batch_features])                \n",
    "                _, train_result, train_loss, train_mmse_loss, train_loss_sum, train_mask_sum = sess.run([train_step, Y_hat, loss, mmse_loss, loss_sum, mask_sum], \n",
    "                                                                                                        feed_dict={X: train_features, Y: train_features},\n",
    "                                                                                                        options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))            \n",
    "                if dense_refeeding:\n",
    "                    _, train_loss, train_mmse_loss, train_loss_sum, train_mask_sum = sess.run([train_step, loss, mmse_loss, loss_sum, mask_sum], \n",
    "                                                                                              feed_dict={X: train_result, Y: train_result})                          \n",
    "                total_loss_sum += train_loss_sum\n",
    "                total_mask_sum += train_mask_sum\n",
    "                train_loss_list.append(train_loss)\n",
    "                train_mmse_loss_list.append(train_mmse_loss) \n",
    "                if np.isnan(train_loss):\n",
    "                    print('Loss Nan error')\n",
    "                    break\n",
    "                if i % 1000 == 0 and i > 0:\n",
    "                    print('Iter: {:4d}, RMSE: {:8f}'.format(i, np.sqrt(total_loss_sum/total_mask_sum)))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Iter: {:4d} is end of line.'.format(i))\n",
    "                break\n",
    "        \n",
    "        trainingRMSE = np.sqrt(total_loss_sum/total_mask_sum)                 \n",
    "        summary = sess.run(merged_summary, feed_dict={X: train_features, Y: train_features}) \n",
    "        train_writer.add_summary(summary, epoch) # epoch * total_train_steps + i        \n",
    "        \n",
    "        sess.run(valid_init_op)\n",
    "        for _ in range(total_valid_steps):\n",
    "            try:\n",
    "                valid_labels, valid_features = sess.run([batch_index, batch_features])\n",
    "                vaild_loss, valid_mmse_loss, valid_loss_sum, valid_mask_sum = sess.run([loss, mmse_loss, loss_sum, mask_sum], \n",
    "                                                                                       feed_dict={X: valid_features, Y: valid_features})\n",
    "                valid_total_loss_sum += valid_loss_sum\n",
    "                valid_total_mask_sum += valid_mask_sum\n",
    "                valid_loss_list.append(vaild_loss)\n",
    "                valid_mmse_loss_list.append(valid_mmse_loss)\n",
    "                if np.isnan(vaild_loss):\n",
    "                    break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break  \n",
    "                \n",
    "        validRMSE = np.sqrt(valid_total_loss_sum/valid_total_mask_sum)\n",
    "        valid_summary = sess.run(merged_summary, feed_dict={X: valid_features, Y: valid_features})            \n",
    "        test_writer.add_summary(valid_summary, epoch)         \n",
    "        #print(\"Epoch: {:>2d}, Train Loss: {:>2.8f}, Valid Loss:{:>2.8f}\".format(epoch, np.mean(train_mmse_loss_list), np.mean(valid_mmse_loss_list)))    \n",
    "        print(\"Epoch: {:>2d}, Train Loss: {:>2.8f}, Valid Loss:{:>2.8f}, Compute time: {:>2.8f}\".format(epoch, trainingRMSE, validRMSE, time.time() - tic))    \n",
    "        \n",
    "        # Early Stopping\n",
    "        if validRMSE < best_valid_loss:\n",
    "            # Update the best-known validation accuracy.\n",
    "            best_valid_loss = valid_mmse_loss\n",
    "            \n",
    "            # Set the iteration for the last improvement to current.\n",
    "            last_improvement = epoch\n",
    "\n",
    "            # Save all variables of the TensorFlow graph to file.\n",
    "            # Build the tensor info             \n",
    "            #graph = tf.get_default_graph()\n",
    "            inputs = g.get_tensor_by_name('Placeholder/Inputs:0')\n",
    "            prediction = g.get_tensor_by_name('Autoencoder/Decoder/Prediction:0')\n",
    "            model_input = tf.saved_model.utils.build_tensor_info(inputs)\n",
    "            model_output = tf.saved_model.utils.build_tensor_info(prediction)\n",
    "            # Build signature definition\n",
    "            signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                inputs={'inputs': model_input},\n",
    "                outputs={'outputs': model_output}, \n",
    "                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "            \n",
    "            if os.path.exists(export_dir):\n",
    "                shutil.rmtree(export_dir, ignore_errors=True)\n",
    "            builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "            builder.add_meta_graph_and_variables(\n",
    "                sess, [tf.saved_model.tag_constants.SERVING], \n",
    "                signature_def_map={\n",
    "                    tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature\n",
    "                }, strip_default_attrs=True)\n",
    "            builder.save()\n",
    "            # Use saver to save graph, variables, assets\n",
    "            #saver.save(sess, os.path.join(ckpt_dir, 'DeepRecommender.ckpt'), \n",
    "            #           global_step=epoch) # epoch * total_train_steps + i\n",
    "\n",
    "            # A string to be printed below, shows improvement found.\n",
    "            improved_str = '*'\n",
    "        else:\n",
    "            # An empty string to be printed below. Shows that no improvement was found.\n",
    "            improved_str = ''\n",
    "            \n",
    "            # Status-message for printing.\n",
    "            #msg = \"Epoch: {0:>6}, Train Loss: {1:>6.5%}, Validation Loss: {2:>6.5%} {3}\"                    \n",
    "            #print(msg.format(epoch, train_mmse_loss, valid_mmse_loss, improved_str))\n",
    "\n",
    "            # If no improvement found in the required number of iterations.\n",
    "            if epoch - last_improvement > require_improvement:\n",
    "                print(\"No improvement found in a while, stopping optimization.\")                \n",
    "                break\n",
    "                            \n",
    "    train_writer.close()  \n",
    "    test_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Saver module to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "save_path = '/home/yuyuliao/tensorflow_ckpt/DeepRcommenderOneAD/'\n",
    "checkpoint_file = tf.train.latest_checkpoint(save_path)\n",
    "chkp.print_tensors_in_checkpoint_file(checkpoint_file, tensor_name='', all_tensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chkp.print_tensors_in_checkpoint_file(checkpoint_file, tensor_name='hyperparameters/W1', all_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "checkpoint_file = tf.train.latest_checkpoint(save_path)\n",
    "#with tf.Session(config=config) as pred_sess:\n",
    "pred_sess = tf.Session(config=config)\n",
    "imported_meta = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "imported_meta.restore(pred_sess, tf.train.latest_checkpoint(save_path))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "graph_nodes_name = [n.name for n in graph.as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_operation_by_name(\"Placeholder/Inputs\"), graph.get_operation_by_name('Autoencoder/Decoder/prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = graph.get_tensor_by_name('Placeholder/Inputs:0')\n",
    "feed_dict = {X: valid_features}\n",
    "output = graph.get_tensor_by_name(\"Autoencoder/Decoder/Prediction:0\")\n",
    "#init = tf.global_variables_initializer()\n",
    "#with tf.Session(graph=graph, config=config) as test_sess: \n",
    "#test_sess.run(init)\n",
    "res = pred_sess.run(output, feed_dict=feed_dict)\n",
    "pred_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SavedModel module to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yuyuliao/tensorflow_ckpt/DeepRcommenderOneAD/export'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_path = Path(HOME) / 'tensorboard_files'\n",
    "ckpt_dir = str(HOME / 'tensorflow_ckpt' / 'DeepRcommenderOneAD')\n",
    "export_dir = os.path.join(ckpt_dir, 'export')\n",
    "export_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'/home/yuyuliao/tensorflow_ckpt/DeepRcommenderOneAD/export/variables/variables'\n",
      "Num of iteration:3709, total predicted count: 474752\n",
      "CPU times: user 7min 16s, sys: 12.5 s, total: 7min 28s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "\n",
    "with tf.Graph().as_default() as g: \n",
    "    test_dataset = load_data(files_list=validation_list, features_name=data_schema['DataSchema'][:-1], \n",
    "                              batch_size=NUM_BATCH_SIZE, epochs=NUM_EPOCHS, is_training=0, name='test') \n",
    "    iterator = tf.data.Iterator.from_structure(test_dataset().output_types, \n",
    "                                               test_dataset().output_shapes)    \n",
    "    testing_batch_index, testing_batch_features = iterator.get_next()\n",
    "    test_init_op = iterator.make_initializer(test_dataset())    \n",
    "\n",
    "    \n",
    "with tf.Session(config=config, graph=g) as pred_sess:\n",
    "    total_valid_steps = 99999\n",
    "    meta_graph_def = tf.saved_model.loader.load(pred_sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n",
    "\n",
    "    signature = meta_graph_def.signature_def\n",
    "    x_tensor_name = signature[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs['inputs'].name\n",
    "    y_tensor_name = signature[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs['outputs'].name\n",
    "\n",
    "    #graph = tf.get_default_graph()\n",
    "    #graph_nodes_name = [n.name for n in graph.as_graph_def().node]\n",
    "    pred_sess.run(test_init_op)\n",
    "    prediction_recoder = []\n",
    "    X = pred_sess.graph.get_tensor_by_name(x_tensor_name)\n",
    "    Y = pred_sess.graph.get_tensor_by_name(y_tensor_name)\n",
    "    for _ in range(total_valid_steps):\n",
    "        try:\n",
    "            testing_labels, testing_features = pred_sess.run([testing_batch_index, testing_batch_features])\n",
    "            decode_valid_labels = [l.decode('utf-8') for l in testing_labels]            \n",
    "            predicted_result = pred_sess.run(Y, feed_dict={X: testing_features})                         \n",
    "            prediction_recoder.append(list(zip(decode_valid_labels, np.round(predicted_result, 6))))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "print('Num of iteration:{}, total predicted count: {}'.format(len(prediction_recoder), \n",
    "                                                              len(prediction_recoder) * NUM_BATCH_SIZE))            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
